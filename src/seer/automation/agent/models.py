from enum import Enum
from typing import Generic, Optional, TypeVar

from pydantic import BaseModel


class ToolCall(BaseModel):
    id: Optional[str] = None
    function: str
    args: str


class Usage(BaseModel):
    completion_tokens: int = 0
    prompt_tokens: int = 0
    total_tokens: int = 0

    def __add__(self, other: "Usage"):
        return Usage(
            completion_tokens=self.completion_tokens + other.completion_tokens,
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            total_tokens=self.total_tokens + other.total_tokens,
        )

    def __sub__(self, other: "Usage"):
        return Usage(
            completion_tokens=self.completion_tokens - other.completion_tokens,
            prompt_tokens=self.prompt_tokens - other.prompt_tokens,
            total_tokens=self.total_tokens - other.total_tokens,
        )


class LlmProviderType(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


class Message(BaseModel):
    content: Optional[str] = None
    """The text of the message."""

    role: str = "user"
    """The role of the author of this message."""

    tool_calls: Optional[list[ToolCall]] = None
    """The tool calls generated by the model, such as function calls."""

    tool_call_id: Optional[str] = None


class LlmResponseMetadata(BaseModel):
    model: str
    provider_name: LlmProviderType
    usage: Usage


class LlmGenerateTextResponse(BaseModel):
    message: Message
    metadata: LlmResponseMetadata


StructuredOutputType = TypeVar("StructuredOutputType")


class LlmGenerateStructuredResponse(BaseModel, Generic[StructuredOutputType]):
    parsed: StructuredOutputType
    metadata: LlmResponseMetadata


class LlmProviderDefaults(BaseModel):
    temperature: float | None = None


class LlmModelDefaultConfig(BaseModel):
    match: str
    defaults: LlmProviderDefaults


class LlmRefusalError(Exception):
    """Raised when the LLM refuses to complete the request."""

    pass
