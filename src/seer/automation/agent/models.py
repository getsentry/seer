import dataclasses
from enum import Enum
from typing import Any, Generic, Optional, TypeVar

from pydantic import BaseModel

DEFAULT_FIRST_TOKEN_TIMEOUT = 40.0
DEFAULT_INACTIVITY_TIMEOUT = 20.0


class ToolCall(BaseModel):
    id: Optional[str] = None
    function: str
    args: str


class Usage(BaseModel):
    completion_tokens: int = 0
    prompt_tokens: int = 0
    total_tokens: int = 0
    prompt_cache_write_tokens: int = 0
    prompt_cache_read_tokens: int = 0

    def __add__(self, other: "Usage"):
        return Usage(
            completion_tokens=self.completion_tokens + other.completion_tokens,
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            total_tokens=self.total_tokens + other.total_tokens,
        )

    def __sub__(self, other: "Usage"):
        return Usage(
            completion_tokens=self.completion_tokens - other.completion_tokens,
            prompt_tokens=self.prompt_tokens - other.prompt_tokens,
            total_tokens=self.total_tokens - other.total_tokens,
        )

    def to_langfuse_usage(self):
        return {
            "prompt_tokens": self.prompt_tokens
            + self.prompt_cache_write_tokens
            + self.prompt_cache_read_tokens,
            "completion_tokens": self.completion_tokens,
            "total_tokens": self.total_tokens,
        }


class LlmProviderType(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GEMINI = "gemini"


class Message(BaseModel):
    content: Optional[str] = None
    """The text of the message."""

    role: str = "user"
    """The role of the author of this message."""

    thinking_content: Optional[str] = None
    """The thinking content of the message (for reasoning models)."""

    thinking_signature: Optional[str] = None
    """The signature of the thinking block (for reasoning models)."""

    tool_calls: Optional[list[ToolCall]] = None
    """The tool calls generated by the model, such as function calls."""

    tool_call_id: Optional[str] = None
    """The ID of the tool call."""

    tool_call_function: Optional[str] = None
    """The function of the tool call."""


class LlmResponseMetadata(BaseModel):
    model: str
    provider_name: LlmProviderType
    usage: Usage
    provider_instance: Any | None = None  # The actual provider instance that was used


class LlmGenerateTextResponse(BaseModel):
    message: Message
    metadata: LlmResponseMetadata


StructuredOutputType = TypeVar("StructuredOutputType")


@dataclasses.dataclass
class LlmGenerateStructuredResponse(Generic[StructuredOutputType]):
    parsed: StructuredOutputType
    metadata: LlmResponseMetadata


class LlmProviderDefaults(BaseModel):
    temperature: float | None = None
    max_tokens: int | None = None
    reasoning_effort: str | None = None
    seed: int | None = None
    timeout: float | None = None
    first_token_timeout: float | None = None
    inactivity_timeout: float | None = None


class ResolvedParameters(BaseModel):
    """
    Resolved parameters after applying precedence: function params > model defaults > provider defaults.
    This model provides type safety and better IDE support compared to using dictionaries.
    """

    temperature: float | None = None
    max_tokens: int | None = None
    reasoning_effort: str | None = None
    timeout: float | None = None
    seed: int | None = None
    first_token_timeout: float = DEFAULT_FIRST_TOKEN_TIMEOUT
    inactivity_timeout: float = DEFAULT_INACTIVITY_TIMEOUT


class LlmModelDefaultConfig(BaseModel):
    match: str
    defaults: LlmProviderDefaults
    region_preference: dict[str, list[str]] | None = None


class LlmRefusalError(Exception):
    """Raised when the LLM refuses to complete the request."""

    pass


class LlmStreamTimeoutError(TimeoutError):
    """Raised when the LLM stream times out."""

    pass


class LlmStreamFirstTokenTimeoutError(LlmStreamTimeoutError):
    """Raised when the LLM takes too long to generate the first token."""

    pass


class LlmStreamInactivityTimeoutError(LlmStreamTimeoutError):
    """Raised when the LLM stream times out due to inactivity."""

    pass


class LlmNoCompletionTokensError(Exception):
    """Raised when the LLM returns no completion tokens."""

    pass
