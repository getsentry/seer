{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create a dataset from Sentry Issues <-> Github commits that reference a sentry issue and save it to langsmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the github client and instantiate the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "from github import Github\n",
    "from github.Auth import Token\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from pydantic import (\n",
    "    AliasChoices,\n",
    "    AliasGenerator,\n",
    "    BaseModel,\n",
    "    ConfigDict,\n",
    "    Field,\n",
    "    ValidationError,\n",
    "    field_validator,\n",
    "    field_serializer\n",
    ")\n",
    "\n",
    "# from pydantic import field_serializer, BaseModel\n",
    "from github.Commit import Commit\n",
    "from typing import Any, Optional\n",
    "from pydantic import ConfigDict, field_validator\n",
    "\n",
    "from seer.automation.autofix.models import IssueDetails\n",
    "from seer.automation.models import EventDetails\n",
    "\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "github = Github(auth=Token(token=os.environ.get('GITHUB_TOKEN')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_resolved_issues(organization_slug=\"sentry\", project_slug=\"sentry\", cursor=None):\n",
    "    url = f\"https://sentry.io/api/0/projects/{organization_slug}/{project_slug}/issues/?query=is:resolved error.type:TypeError\"\n",
    "\n",
    "    # if cursor: \n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {os.environ.get('SENTRY_AUTH_TOKEN')}\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    result = response.json()\n",
    "\n",
    "    if \"detail\" in result:\n",
    "        raise Exception(result[\"detail\"])\n",
    "\n",
    "    return result, response.links[\"next\"]\n",
    "\n",
    "def auth_headers(auth_token=None, auth_cookie=None):\n",
    "    auth_token = auth_token if auth_token else os.environ.get('SENTRY_AUTH_TOKEN')\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {auth_token}\"\n",
    "    }\n",
    "    if auth_cookie:\n",
    "        headers[\"Cookie\"] = auth_cookie\n",
    "    return headers    \n",
    "\n",
    "def get_issue_by_id(issue_id, organization_slug=\"sentry\", auth_token=None, auth_cookie=None):\n",
    "    url = (\n",
    "        f\"https://sentry.io/api/0/organizations/{organization_slug}/issues/{issue_id}/\"\n",
    "    )\n",
    "\n",
    "    headers = auth_headers(auth_token, auth_cookie)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    issue = response.json()\n",
    "\n",
    "    if \"detail\" in issue and issue[\"detail\"] == \"The requested resource does not exist\":\n",
    "        raise Exception(f\"Could not find issue with id {issue_id}\")\n",
    "\n",
    "    return issue\n",
    "\n",
    "\n",
    "def get_issue_id_from_short_id(short_id, organization_slug=\"sentry\", auth_token=None, auth_cookie=None):\n",
    "    url = f\"https://sentry.io/api/0/organizations/{organization_slug}/shortids/{short_id}/\"\n",
    "    headers = auth_headers(auth_token, auth_cookie)\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    result = response.json()\n",
    "\n",
    "    if ( \n",
    "        \"detail\" in result\n",
    "        and result[\"detail\"] == \"The requested resource does not exist\"\n",
    "    ):\n",
    "        raise Exception(f\"Could not find issue with short id {short_id}\")\n",
    "\n",
    "    return result[\"groupId\"]\n",
    "\n",
    "\n",
    "def get_details_for_issue(issue_id=None, short_id=None, organization_slug=\"sentry\", auth_token=None, auth_cookie=None):\n",
    "    if issue_id is None and short_id is None:\n",
    "        raise Exception(\"Either issue_id or short_id must be provided\")\n",
    "\n",
    "    if short_id:\n",
    "        issue_id = get_issue_id_from_short_id(short_id, organization_slug, auth_token, auth_cookie)\n",
    "    issue = get_issue_by_id(issue_id, organization_slug, auth_token, auth_cookie)\n",
    "    \n",
    "    if 'id' not in issue:\n",
    "        if issue['detail'] == 'You do not have permission to perform this action.':\n",
    "            # Its possible that the token is expired. Prompt for token and retry\n",
    "            auth_cookie = input('Sentry sudo cookie')\n",
    "            if short_id:\n",
    "                issue_id = get_issue_id_from_short_id(short_id, organization_slug, auth_token, auth_cookie)\n",
    "            issue = get_issue_by_id(issue_id, organization_slug, auth_token, auth_cookie)\n",
    "            if 'id' not in issue:\n",
    "                raise Exception(issue)\n",
    "        else:\n",
    "            raise Exception(issue)\n",
    "            \n",
    "    url = f\"https://sentry.io/api/0/organizations/{organization_slug}/issues/{issue['id']}/events/?full=true\"\n",
    "    headers = auth_headers(auth_token, auth_cookie)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    events = response.json()\n",
    "    return auth_cookie, dict(\n",
    "        **issue,\n",
    "        events=events[:1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_details_for_issue(issue_id=5206388570, organization_slug='seria-ati'))\n",
    "print(get_details_for_issue(issue_id=5177147602, organization_slug='sentry'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalItem(BaseModel):\n",
    "    raw_data: dict[str, Any]\n",
    "    organization_id: int\n",
    "    project_id: int\n",
    "    repo_name: Optional[str] = None\n",
    "    commit_hash: Optional[str] = None\n",
    "    # Field order matters as commit is dependent on repo_name and commit_hash, it should come later down the order.\n",
    "    commit: Commit | str\n",
    "    issue: IssueDetails\n",
    "    event: EventDetails\n",
    "    \n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "    \n",
    "    @field_serializer('commit')\n",
    "    def serialize_commit(self, commit: Commit, _info):\n",
    "        return commit.sha\n",
    "    \n",
    "    @field_validator('commit', mode=\"after\")\n",
    "    @classmethod\n",
    "    def validate_commit(cls, commit: Commit | str, values, **kwargs):\n",
    "        if isinstance(commit, Commit):\n",
    "            return commit\n",
    "        if 'repo_name' in values.data and values.data['repo_name'] is not None :\n",
    "            repo_name = values.data['repo_name']\n",
    "        else:\n",
    "            repo_name = 'getsentry/sentry'\n",
    "            values.data['repo_name'] = repo_name\n",
    "        repo = github.get_repo(repo_name)\n",
    "        values.data['commit_hash'] = commit\n",
    "        return repo.get_commit(commit)\n",
    "        \n",
    "class EvalItemWithDiff(EvalItem):\n",
    "    diff: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits(repo, since):\n",
    "    \"\"\"\n",
    "        Get all the commits from repo for a timeframe.\n",
    "    \"\"\"\n",
    "    days_ago = datetime.datetime.now() - timedelta(days=since)\n",
    "    print('Querying for commits')\n",
    "    all_commits = repo.get_commits(since=days_ago)\n",
    "    # all_commits = [commit for commit in commits]\n",
    "    print('Total commits in this timeframe: ', all_commits.totalCount)\n",
    "    return all_commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commits_with_sentry_issue(all_commits):\n",
    "    \"\"\"\n",
    "        Filter it down to only commits with sentry issues.\n",
    "    \"\"\"\n",
    "    # Gets the commits with an id or url to a sentry issue\n",
    "    with_id_or_url = []\n",
    "\n",
    "    with tqdm(all_commits, total=all_commits.totalCount, desc='Find Commits That Fix Issues', unit='Commit') as pbar:\n",
    "        with tqdm(desc='Positive', unit='Commit') as ctr1:\n",
    "            with tqdm(desc='Negative', unit='Commit') as ctr2:\n",
    "                for commit in pbar:\n",
    "                    if 'SENTRY-' in commit.commit.message or 'https://sentry.sentry.io/issues/' in commit.commit.message:\n",
    "                        # Extracts the short id or id from the commit message\n",
    "                        message = commit.commit.message\n",
    "                        issue_short_id_match = re.findall(r'SENTRY-.{4}', message)\n",
    "                        issue_short_id = issue_short_id_match[0] if issue_short_id_match else None\n",
    "                        issue_url = re.findall(r'https://sentry.sentry.io/issues/\\d+', message)\n",
    "                        issue_id = issue_url[0].split('/')[-1] if issue_url else None                \n",
    "                        if issue_short_id or issue_id:\n",
    "                            with_id_or_url.append((1, 1, 'getsentry/sentry', commit.sha, commit, issue_short_id, issue_id, 'sentry'))\n",
    "                            ctr1.update(1)\n",
    "                        else:\n",
    "                            ctr2.update(1)\n",
    "                    else:\n",
    "                        ctr2.update(1)\n",
    "                        \n",
    "    return with_id_or_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_items_for_sentry(with_id_or_url, auth_token=None, auth_cookie=None):\n",
    "    \"\"\"\n",
    "        Populate into eval items.\n",
    "    \"\"\"\n",
    "    eval_items: list[EvalItem] = []\n",
    "    skipped_items: list[EvalItem] = []\n",
    "    error_count = 0\n",
    "    errors = []\n",
    "    prev_auth_cookie = auth_cookie\n",
    "    with tqdm(total=len(with_id_or_url), desc='Get issue details') as pbar: \n",
    "        for org_id, project_id, repo_name, commit_hash, commit, short_id, issue_id, org_slug in with_id_or_url:\n",
    "            try:\n",
    "                auth_cookie, issue = get_details_for_issue(issue_id=issue_id, short_id=short_id, organization_slug=org_slug, auth_token=auth_token, auth_cookie=auth_cookie)\n",
    "                if auth_cookie != prev_auth_cookie:\n",
    "                    print(\"Cookie changed\")\n",
    "                    prev_auth_cookie = auth_cookie\n",
    "                issue_details = IssueDetails.model_validate(issue)\n",
    "                event_details = EventDetails.from_event(issue_details.events[0])\n",
    "    \n",
    "                eval_item = EvalItem(\n",
    "                    organization_id=org_id,\n",
    "                    project_id=project_id,\n",
    "                    repo_name=repo_name,\n",
    "                    commit_hash=commit_hash,\n",
    "                    commit=commit,\n",
    "                    raw_data=issue,\n",
    "                    issue=issue_details,\n",
    "                    event=event_details\n",
    "                )\n",
    "    \n",
    "                if len(event_details.exceptions) == 0:\n",
    "                    skipped_items.append(eval_item)\n",
    "                    continue\n",
    "    \n",
    "                eval_items.append(eval_item)\n",
    "            except Exception as e:\n",
    "                if 'You do not have permission to perform this action.' in repr(e):\n",
    "                    abort = input(\"Auth token is not working. Abort (yes/no/retry)?\")\n",
    "                    if abort.lower() == 'yes':\n",
    "                        break\n",
    "                else:\n",
    "                    print(repr(e))\n",
    "                errors.append({type(e):e})\n",
    "                error_count += 1\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print('Total eval items:', len(eval_items))\n",
    "    print('Total skipped items (no exceptions in event details):', len(skipped_items))\n",
    "    print('Total errors:', error_count)\n",
    "    if len(errors) > 0:\n",
    "        print('Errors:')\n",
    "        print('------------------------------')\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "            print('------------------------------')\n",
    "    return eval_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from github.Commit import Commit\n",
    "from github.File import File\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
    "\n",
    "# Methods for Prompt GPT so we can filter it down to only issues that are \"actionable\" \n",
    "# which means in this case, given a sentry issue, it should be evident \n",
    "# what the developer should do to fix it.\n",
    "\n",
    "def file_patch_to_str(file: File):\n",
    "    return f\"[{file.filename}]\\n{file.patch}\"\n",
    "\n",
    "\n",
    "def explain_changes(error_msg, stack_str, commit_message, files_str):\n",
    "    response = model.invoke(\n",
    "        f\"\"\"<error_message>\n",
    "{error_msg}\n",
    "</error_message>\n",
    "<stacktrace>\n",
    "{stack_str}\n",
    "</stacktrace>\n",
    "\n",
    "A software engineer then created the following changes in a commit to fix the above issue:\n",
    "<commit_message>\n",
    "{commit_message}\n",
    "</commit_message>\n",
    "<changes>\n",
    "{files_str}\n",
    "</changes>\n",
    "\n",
    "How would you describe the solution to the error in a short summary. Also describe what the root cause of the problem ended up being.\"\"\"\n",
    "    )\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def determine_actionability(error_msg, stack_str, solution):\n",
    "    response = model.invoke(\n",
    "        f\"\"\"Given the provided information:\n",
    "<information>\n",
    "<error_message>\n",
    "{error_msg}\n",
    "</error_message>\n",
    "<stacktrace>\n",
    "{stack_str}\n",
    "</stacktrace>\n",
    "</information>\n",
    "\n",
    "<expected_solution>\n",
    "{solution}\n",
    "</expected_solution>\n",
    "\n",
    "Based on the error message and stacktrace, can the solution be inferred from the information given and access to reading the codebase? Why or why not?\n",
    "Answer in the format:<response>yes/no</response><reason>reason for the response</reason>\"\"\"\n",
    "    )\n",
    "    comatch = re.match(r\"<response>(.*?)</response>\", response.content)\n",
    "    if comatch and \"yes\" in comatch.group(1).lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_fixable_items(eval_items):\n",
    "    fixable_items: list[EvalItem] = []\n",
    "    panel_n = 5\n",
    "    with tqdm(eval_items, desc='Check fixability', total=len(eval_items)) as pbar:\n",
    "        with tqdm(desc='Fixable') as ctr1:\n",
    "            with tqdm(desc='Not Fixable') as ctr2:\n",
    "                for eval_item in pbar:\n",
    "                    issue_details = eval_item.issue\n",
    "                    stacktrace = eval_item.event.exceptions[0].stacktrace            \n",
    "                    stacktrace_str = stacktrace.to_str(max_frames=64)\n",
    "                    commit = eval_item.commit\n",
    "                    files = commit.files\n",
    "                    files_str = \"\\n\".join([file_patch_to_str(file) for file in files])\n",
    "            \n",
    "                    explain_result = explain_changes(\n",
    "                        issue_details.title, stacktrace_str, commit.commit.message, files_str\n",
    "                    )\n",
    "            \n",
    "                    actionability_results = []\n",
    "                    final_result = False\n",
    "                    for _ in range(panel_n):\n",
    "                        actionability_result = determine_actionability(issue_details.title, stacktrace_str, explain_result)\n",
    "                        actionability_results.append(actionability_result)\n",
    "                        true_count = actionability_results.count(True)\n",
    "                        false_count = actionability_results.count(False)\n",
    "                        if true_count > panel_n / 2:\n",
    "                            final_result = True\n",
    "                            break\n",
    "                        if false_count > panel_n / 2:\n",
    "                            final_result = False\n",
    "                            break\n",
    "                    \n",
    "                    if final_result:\n",
    "                        fixable_items.append(eval_item)\n",
    "                        ctr1.update(1)\n",
    "                    else:\n",
    "                        ctr2.update(1)\n",
    "                        \n",
    "    print('Total fixable items:', len(fixable_items))\n",
    "    print('Total non-fixable items:', len(eval_items) - len(fixable_items))\n",
    "    return fixable_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_diff(fixable_items):\n",
    "    \"\"\"\n",
    "        Populate the eval items that are fixable with their expected diffs\n",
    "    \"\"\"\n",
    "    final_eval_items: list[EvalItemWithDiff] = []\n",
    "    for item in tqdm(fixable_items, desc='Loading Diff Info'):\n",
    "        repo = github.get_repo(item.repo_name)\n",
    "        comparison = repo.compare(item.commit.commit.parents[0].sha, item.commit.sha)\n",
    "        \n",
    "        requester = repo._requester\n",
    "        headers = {\n",
    "            \"Authorization\": f\"{requester._Requester__auth.token_type} {requester._Requester__auth.token}\",  # type: ignore\n",
    "            \"User-Agent\": requester._Requester__userAgent,  # type: ignore\n",
    "        }\n",
    "        diff_data = requests.get(comparison.diff_url, headers=headers).content.decode('utf-8')\n",
    "    \n",
    "        final_item = EvalItemWithDiff.model_validate(dict(\n",
    "            **dict(item),\n",
    "            diff=diff_data\n",
    "        ))\n",
    "        final_eval_items.append(final_item)\n",
    "    return final_eval_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dump_items(items: list[EvalItem], filename: str):\n",
    "    serialized_items = [item.model_dump(mode='json') for item in items]\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(serialized_items, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fixable Items From Sentry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixable_issues_from_sentry(since=90, op_file='../data/eval_items.json'):\n",
    "    repo = github.get_repo('getsentry/sentry')\n",
    "    all_commits = get_commits(repo, since)\n",
    "    with_id_or_url = commits_with_sentry_issue(all_commits)\n",
    "    eval_items = eval_items_for_sentry(with_id_or_url)\n",
    "    fixable_items = get_fixable_items(eval_items)\n",
    "    final_eval_items = add_diff(fixable_items)\n",
    "    print('Total final eval items:', len(final_eval_items))\n",
    "    dump_items(final_eval_items, op_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "def delete_if_exists(client, dataset_name):\n",
    "    if client.has_dataset(dataset_name=dataset_name):\n",
    "        deleted = False\n",
    "        print(f'Dataset {dataset_name} exists already. Clearing it first.')\n",
    "        for cur in client.list_datasets():\n",
    "            if cur.name == dataset_name:\n",
    "                client.delete_dataset(dataset_id=str(cur.id))\n",
    "                deleted = True\n",
    "        if not deleted:\n",
    "            raise Exception('Failed to find the dataset to delete')\n",
    "\n",
    "def create_langsmith_dataset(items, num_entries, dataset_name, description, overwrite=False):    \n",
    "    client = Client()\n",
    "    if overwrite:\n",
    "        delete_if_exists(client, dataset_name)\n",
    "        \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=description)\n",
    "    errors = []\n",
    "    with tqdm(desc='Uploading Example', total=num_entries) as pbar:\n",
    "        with tqdm(desc='Errors') as ctr1:\n",
    "            uploaded = 0\n",
    "            cur_index = 0\n",
    "            while uploaded < num_entries and cur_index < len(items):\n",
    "                item = EvalItemWithDiff.model_validate(items[cur_index])\n",
    "                cur_index = cur_index + 1\n",
    "                input = item.model_dump(mode='json')\n",
    "                output = { \"diff\": item.diff }\n",
    "                try:            \n",
    "                    client.create_example(\n",
    "                        inputs=input, \n",
    "                        outputs=output,\n",
    "                        dataset_id=dataset.id)\n",
    "                    pbar.update(1)\n",
    "                    uploaded = uploaded + 1\n",
    "                except Exception as e:\n",
    "                    ctr1.update(1)\n",
    "                    errors.append({type(e):e})\n",
    "\n",
    "    print(f'Uploaded {uploaded} samples to dataset')\n",
    "    if len(errors) > 0:\n",
    "        print('-------------Errors-------------')\n",
    "        for e in errors:\n",
    "            print(e, '----------------------')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_langsmith_dataset(\n",
    "#     final_eval_items, \n",
    "#     \"Autofix Eval Full 240314\", \n",
    "#     \"Autofix full eval made from mapping sentry <-> github commits for sentry project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fixable Items From Issues Related To Open Source Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bigquery_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def os_commits_with_sentry_issue(sentry_org=False, limit=None):\n",
    "    org_predicate = 'organization_id = 1 AND project_id = 1' if sentry_org else 'organization_id <> 1'\n",
    "    limit_clause = f'LIMIT {limit}' if limit else ''\n",
    "    \n",
    "         \n",
    "    bq_sql = f\"\"\"\n",
    "        WITH status_info AS (\n",
    "          SELECT * FROM getsentry.sentry_grouphistory\n",
    "          WHERE status IN (12, 13)\n",
    "            AND {org_predicate}\n",
    "            -- AND organization_id <> 1\n",
    "            -- AND organization_id = 1 AND project_id = 1\n",
    "        ),\n",
    "        commit_ids AS (\n",
    "          SELECT group_id, linked_id, project_id\n",
    "          FROM getsentry.sentry_grouplink \n",
    "          WHERE linked_type = 1 AND relationship = 1 \n",
    "            AND group_id IN (SELECT distinct group_id FROM status_info)\n",
    "        )\n",
    "        SELECT commits.organization_id, \n",
    "          org.name as organization_name, org.slug as organization_slug,\n",
    "          commit_ids.project_id, commit_ids.group_id, repos.name, \n",
    "          commits.author_id, commits.date_added, \n",
    "          commits.key, commits.message, \n",
    "          commits.repository_id\n",
    "        FROM getsentry.sentry_commit AS commits\n",
    "        JOIN getsentry.sentry_repository AS repos\n",
    "          ON commits.organization_id = repos.organization_id AND commits.repository_id = repos.id\n",
    "        JOIN `tmp_ram.github_open_source_repos` AS oss_repos \n",
    "          ON repos.name = oss_repos.name\n",
    "        JOIN commit_ids \n",
    "          ON  commits.id = commit_ids.linked_id\n",
    "        JOIN `getsentry.sentry_organization` AS org\n",
    "          ON commits.organization_id = org.id  \n",
    "        WHERE commits.id IN (SELECT distinct linked_id FROM commit_ids) ORDER BY date_added DESC\n",
    "        -- LIMIT 5\n",
    "        {limit_clause}\n",
    "    \"\"\"\n",
    "    results = bigquery_client.query(bq_sql).to_dataframe()\n",
    "    print(f'Retrieved {results.shape[0]} commits associated with resolved issues')\n",
    "    with_id_or_url = []\n",
    "    failed = []\n",
    "\n",
    "    with (\n",
    "        tqdm(results.iterrows(), total=results.shape[0]) as pbar,\n",
    "        tqdm(desc='Successful') as ctr1,\n",
    "        tqdm(desc='Failed') as ctr2):\n",
    "        for i, row in pbar:\n",
    "            cur_hash = row['key']\n",
    "            repo_name = row['name']\n",
    "            pbar.set_description(f'Commit {cur_hash[0:8]} from {repo_name}')\n",
    "            try:\n",
    "                repo = github.get_repo(repo_name)\n",
    "                commit = repo.get_commit(cur_hash)\n",
    "                with_id_or_url.append((row['organization_id'], row['project_id'], repo_name, cur_hash, commit, None, row['group_id'], row['organization_slug']))\n",
    "                ctr1.update(1)\n",
    "            except Exception as e:\n",
    "                failed.append([repo_name, cur_hash, e])\n",
    "                ctr2.update(1)\n",
    "\n",
    "    if len(failed) > 0:\n",
    "        print('Errors:')\n",
    "        print('-----------------------------')\n",
    "        for repo_name, cur_hash, e in failed:\n",
    "            print(f'Error getting commit details for {cur_hash} from repo {repo_name}: {e}')\n",
    "            print('-----------------------------')\n",
    "        \n",
    "    return with_id_or_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from(filenames, items_have_diff=True, add_diff_if_missing=True):\n",
    "    all_items = []\n",
    "    for filename in filenames:\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "            if items_have_diff:\n",
    "                items = [EvalItemWithDiff.model_validate(cur) for cur in tqdm(data, desc='Validating Data')]\n",
    "            else:\n",
    "                items = [EvalItem.model_validate(cur) for cur in tqdm(data, desc='Validating Data')]\n",
    "                if add_diff_if_missing:\n",
    "                    items = add_diff(items)\n",
    "            all_items = all_items + items\n",
    "    return all_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_os_fixable_issues(op_file='../data/eval_items.json', eval_items_file=None, needs_su=False, sentry_org=False, limit=None, load_cached=False, skip_fixable_check=False):\n",
    "    if load_cached:\n",
    "        #load from eval_items_file\n",
    "        eval_items = load_data_from([eval_items_file], items_have_diff=False, add_diff_if_missing=False)\n",
    "    else:\n",
    "        with_id_or_url = os_commits_with_sentry_issue(sentry_org, limit)\n",
    "        if needs_su:\n",
    "            eval_items = eval_items_for_sentry(with_id_or_url, auth_token=None, auth_cookie='dummy_cookie')\n",
    "        else:\n",
    "            eval_items = eval_items_for_sentry(with_id_or_url)\n",
    "        if eval_items_file:\n",
    "            print(f'Saving intermediate results (before running ChatGPT based filtering) to {eval_items_file}.')\n",
    "            dump_items(eval_items, eval_items_file)\n",
    "    if not skip_fixable_check:\n",
    "        fixable_items = get_fixable_items(eval_items)\n",
    "        final_eval_items = add_diff(fixable_items)\n",
    "        print('Total final eval items:', len(final_eval_items))\n",
    "        dump_items(final_eval_items, op_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Issues And Commits From Sentry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762c1445c0fb4ea08e48e15470b3fcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Data:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52df1d2f362e4f259eb78a8b797670b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Check fixability:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8544580faa08442b98db092894def50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fixable: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985969c41ab14609a82ab9ce04d4198b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Not Fixable: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fixable items: 88\n",
      "Total non-fixable items: 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7585c5ecf06645648b049b61382f358a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Diff Info:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total final eval items: 88\n"
     ]
    }
   ],
   "source": [
    "# Step 1: load data from sentry, filter and save to intermediate file.\n",
    "# get_os_fixable_issues(op_file='../data/eval_sentry_items_from_db_apr_23_90_days.json', \n",
    "#                       eval_items_file='../data/inter_eval_sentry_items_from_db_apr_23_90_days.json',\n",
    "#                       needs_su=False, sentry_org=True, limit=None, \n",
    "#                       load_cached=False, skip_fixable_check=True)\n",
    "\n",
    "# Step 2: Load from intermendiate file, check if fixable using ChatGPT and save\n",
    "# get_os_fixable_issues(op_file='../data/eval_sentry_items_from_db_apr_23_90_days.json', \n",
    "#                       eval_items_file='../data/inter_eval_sentry_items_from_db_apr_23_90_days.json',\n",
    "#                       needs_su=False, sentry_org=True, limit=None, load_cached=True, skip_fixable_check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Issues And Commits From Open Source Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafb076684164d9d8d70674b6c733e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Data:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb0ff517973471a8132fa81fe6d268d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Check fixability:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b7efa9da404f61adebe2ed4184f536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fixable: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c73bd82c7d6472db69e66b5e64018ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Not Fixable: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fixable items: 99\n",
      "Total non-fixable items: 90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1ea4682aa94ea6a2827b886f4bcc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Diff Info:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total final eval items: 99\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: load data from sentry, filter and save to intermediate file.\n",
    "# get_os_fixable_issues(op_file='../data/eval_os_items_from_db_apr_23_90_days.json', \n",
    "#                       eval_items_file='../data/inter_eval_os_items_from_db_apr_23_90_days.json',\n",
    "#                       needs_su=True, sentry_org=False, limit=None,\n",
    "#                       load_cached=False, skip_fixable_check=True)\n",
    "\n",
    "# Step 2: Load from intermendiate file, check if fixable using ChatGPT and save\n",
    "# get_os_fixable_issues(op_file='../data/eval_os_items_from_db_apr_23_90_days.json', \n",
    "#                       eval_items_file='../data/inter_eval_os_items_from_db_apr_23_90_days.json',\n",
    "#                       needs_su=True, sentry_org=False, limit=None,\n",
    "#                       load_cached=True, skip_fixable_check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Presaved JSON Data To LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_sample(all_items, num_entries):\n",
    "    indices = list(range(len(all_items)))\n",
    "    random.shuffle(indices)\n",
    "    selected = []\n",
    "    for idx in indices[0:num_entries]:\n",
    "        selected.append(all_items[idx])\n",
    "    return selected\n",
    "    \n",
    "def save_langsmith(ds_name, filenames, num_entries=None, shuffle=True, overwrite=False):\n",
    "    all_items = []\n",
    "    for filename in filenames:\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "            all_items = all_items + data\n",
    "    \n",
    "    print(f'Loaded {len(all_items)} items')\n",
    "    if shuffle:\n",
    "        random.shuffle(all_items)\n",
    "        \n",
    "    create_langsmith_dataset(\n",
    "        all_items,\n",
    "        num_entries,\n",
    "        ds_name,\n",
    "        f\"{num_entries} issues with related github commits for autofix validation\",\n",
    "        overwrite=overwrite)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create The Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 187 items\n",
      "Dataset Autofix Eval 100 240423 exists already. Clearing it first.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdfcd493738452698c0552721d9d3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Example:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6a3bf5d44f458ab579d06cee5ba678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Errors: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 100 samples to dataset\n",
      "-------------Errors-------------\n",
      "{<class 'requests.exceptions.HTTPError'>: HTTPError('500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/examples', '{\"detail\":\"Internal server error\"}')} ----------------------\n"
     ]
    }
   ],
   "source": [
    "save_langsmith(\n",
    "    ds_name=\"Autofix Eval 100 240423\", \n",
    "    filenames=['../data/eval_os_items_from_db_apr_23_90_days.json', '../data/eval_sentry_items_from_db_apr_23_90_days.json'], \n",
    "    num_entries=100,\n",
    "    shuffle=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Smaller Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 187 items\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd031f298874654972d5edab6cec66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Example:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca284cf82dc40498171db8c166618bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Errors: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 3 samples to dataset\n"
     ]
    }
   ],
   "source": [
    "save_langsmith(\n",
    "    ds_name=\"Autofix Eval 3 240423\", \n",
    "    # filenames=['../data/deleteme_three.json']\n",
    "    # filenames=['../data/eval_os_items_from_db_apr_23_90_days.json'],\n",
    "    # filenames=['../data/oss_one.json'],\n",
    "    filenames=['../data/eval_os_items_from_db_apr_23_90_days.json', '../data/eval_sentry_items_from_db_apr_23_90_days.json'], \n",
    "    num_entries=3,\n",
    "    shuffle=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab24031ad2f47d3b4666602dda65b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Data:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# save_langsmith(ds_name='small-sentry', filenames=['../data/deleteme_three.json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba18f636cf24c558d6da913dc1aa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Data:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f806fa46800b48c0b1aa65d1c36fcbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Data:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{('06abdc1cbde8429a72c24e1aa1d38c29213dc3e0', 'getsentry/sentry'),\n",
       " ('09e8a788e756b0267fbd9b9615dfa3b8d3bfa3e4', 'seriaati/hoyo-buddy'),\n",
       " ('0b7934a41977af9dc83b4449cbdc644ea9914eab', 'FireDiscordBot/bot'),\n",
       " ('0ca66611fbc27cdb10ba0b5fc5b0b3549f86042b', 'getsentry/sentry'),\n",
       " ('0cd5bf636e6033076a2782bbf67db890a9084a8a', 'getsentry/sentry'),\n",
       " ('0e1b234d6876fb16835268407adb418166b7d502', 'renalreg/ukrdc-nuxt-3'),\n",
       " ('0fc53e9a2bd17c2aca3108ef1ded78e41b4fb3f6', 'internetstandards/Internet.nl'),\n",
       " ('14c3d20a481950f5eed83e5c832eda93ea36133c', 'freelawproject/courtlistener'),\n",
       " ('16104c07186d2d9fc10c0418e829597b529e8d94', 'getsentry/sentry'),\n",
       " ('167faedb0522fdb550a0c61c42a1ab5b2c8ecadd', 'getsentry/sentry'),\n",
       " ('16845a9456e4c4fe309e8fe48d71198fa275ba72', 'seriaati/hoyo-buddy'),\n",
       " ('181e6bea052b4e0efc0bc1bea8ab3f26e1882b3e', 'bfkeinberg/route-forecast'),\n",
       " ('185c973b9720bab11864e3917c8faff8f94b0207', 'getsentry/sentry'),\n",
       " ('196615a59a0eb5d7498c39208e0b786667243ce9', 'YodaBotOS/YodaBot'),\n",
       " ('19c9b4e61be9841604e940439fde981a2fd55c1b', 'mano-sesan/mano'),\n",
       " ('1ac5126b0e11d41c7f2f178da1b570c136a043a2', 'getsentry/sentry'),\n",
       " ('1c9c239fd6bf84dce6d01a5221ba45f3d98b135f', 'FireDiscordBot/bot'),\n",
       " ('1f35925345c1340df8a665f322b4afab52ad59af', 'webkom/lego-webapp'),\n",
       " ('2413371070e100302e0070b9843a763f8cdc3191', 'getsentry/sentry'),\n",
       " ('26beeff3fe3dc2c86c226ed7a7188dec8db924a3', 'getsentry/sentry'),\n",
       " ('2906d166001c179168863ebd8f2738def8f1fc54', 'getsentry/sentry'),\n",
       " ('293aba6ab111d00dbf60701d6ab3c27ed5979e55', 'getsentry/sentry'),\n",
       " ('29c8919ec3f93c70323f71f493b198a6cb4e7058', 'getsentry/sentry'),\n",
       " ('2b30f01dd95447933bdbc01de1ae652987475ce9', 'getsentry/sentry'),\n",
       " ('2bfa5621f00ca884796c1843f957f7ad8a3c672b', 'getsentry/sentry'),\n",
       " ('2c4527335a3265740c714f19d85abb34cb30d197', 'getsentry/sentry'),\n",
       " ('2c514900c53b31a086449aede3c151e4023b84eb', 'Ultimaker/Uranium'),\n",
       " ('2ce41ab5c451c0e29cc2e4c3a6c196a5298f222b', 'getsentry/sentry'),\n",
       " ('31b32d6a4589a6643f1547e462040c9f30d40cf8', 'getsentry/sentry'),\n",
       " ('3418bf2df2d422e0e83b0d4844fa2ba3f2321cac', 'FireDiscordBot/bot'),\n",
       " ('360658f6ffb648586e5c66b9ab2d4bc83d8a6513', 'FireDiscordBot/bot'),\n",
       " ('366938990e20329933236a88c4ee0499f2690ed5', 'liberapay/liberapay.com'),\n",
       " ('3d2496297337b3c5018b8fd7fa18cab3820022dc', 'getsentry/sentry'),\n",
       " ('40d2f44cdefe63e2070ea0157008de9bc3756855', 'getsentry/sentry'),\n",
       " ('4220be409e84f4bec1cbb4d8aefa69ed2f46787a', 'getsentry/sentry'),\n",
       " ('45e46ee75227aea2b5609438fbfafe38218a8cfb', 'argos-ci/argos'),\n",
       " ('465921ea4dbf001890688f2927255f27801c2a24', 'getsentry/sentry'),\n",
       " ('468d67ec8d135fa0e69615db9f03dffbfb42a440', 'bfkeinberg/route-forecast'),\n",
       " ('49dea17b43f289ab4918aec9130223978dc05762', 'liberapay/liberapay.com'),\n",
       " ('4b1a1dbd5ee8172523c51bc8d902f0226e310b85', 'FireDiscordBot/bot'),\n",
       " ('4b1c025fec6b0fcaa8cc16f074332439e95a434e', 'inikoo/aurora'),\n",
       " ('4cd5c2fcff19679d49e3a0af4c72519c9fd4abe3', 'getsentry/sentry'),\n",
       " ('4d2f310a5553a7dafbdb102c678954a43b1f2556', 'getsentry/sentry'),\n",
       " ('4f323d06288cfdd68b70a9d192794822d1d48c0d', 'getsentry/sentry'),\n",
       " ('5193e81659df57d96d4b365f5a0ca6fc25550b3a', 'seriaati/hoyo-buddy'),\n",
       " ('5208d645a686f2fa1f121b7f2c0f253ee2a4d79e', 'getsentry/sentry'),\n",
       " ('536c531aa31321fb567655ba39014888dad84d98', 'cowprotocol/cowswap'),\n",
       " ('540037d66a56ff2eae4efa3ee3be4744e80c5554', 'freelawproject/courtlistener'),\n",
       " ('5426d4951f6c65a3a54f70f907dc8f784304702c', 'inikoo/aurora'),\n",
       " ('550902c975ab8263bf749fbd304ff8f7ae09bb57', 'getsentry/sentry'),\n",
       " ('55ee5993c51b15901357344f034f45ea0320090c', 'getsentry/sentry'),\n",
       " ('569bec562cf984b864f6779edaf95e1fb7b63fe8', 'bfkeinberg/route-forecast'),\n",
       " ('576832f90d1c4f4261fae821c050eb15d5ea4bd1', 'seriaati/hoyo-buddy'),\n",
       " ('583dae2e31b34f921b26f04c15fbcfcac75bca18', 'getsentry/sentry'),\n",
       " ('590be0615a8d75b2faf23df07dcdac2fed85c409', 'FireDiscordBot/bot'),\n",
       " ('59274db95a41db3d13084733d3d62959804a3163', 'Belphemur/SoundSwitch'),\n",
       " ('59595bae82cbb7f753c52e728ce6d1d9d5ce1aa2', 'FireDiscordBot/bot'),\n",
       " ('596dd3fc6a3e2af212071dda6c1624420209e0cb', 'getsentry/sentry'),\n",
       " ('5a92b4a3fdb90f8bd051b97dc59d3f5effc2201c',\n",
       "  'HearthSim/Hearthstone-Deck-Tracker'),\n",
       " ('5affb9f8d7dbdf8f349c0c8b5f6ae472fe4f94fe', 'getsentry/sentry'),\n",
       " ('5db8b1f990a9b72457b36fae16d8ea5bedc4ba61', 'getsentry/sentry'),\n",
       " ('61bb14a487d075865f295dce295c1c37575c77a1', 'ShokoAnime/ShokoServer'),\n",
       " ('648216e29776219e11dc16d4dac7136dbb43668e', 'getsentry/sentry'),\n",
       " ('64bbcdfcff8726157b13f8e40f9de179905db5ff', 'getsentry/sentry'),\n",
       " ('65fc9d745e2fefd5f7b670475c1969633bfdea75', 'internetstandards/Internet.nl'),\n",
       " ('69e2a344f8c43fc69bb4502a20862dd79cae36c1', 'getsentry/sentry'),\n",
       " ('6a79ec9f03c4f3397c8bb32bbb655bc85865cec0', 'Belphemur/SoundSwitch'),\n",
       " ('6e9d6a1e579402c70a40fcf6d0d5838b31737b67', 'argos-ci/argos'),\n",
       " ('6ebd3241156959410cda52471da9447c9be2efba', 'getsentry/sentry'),\n",
       " ('70ce0b0206080f36b0a0320decf9e020ec4802db', 'getsentry/sentry'),\n",
       " ('71bbc8b7a235a1c908b571645bdd9a3c12c5963a', 'internetstandards/Internet.nl'),\n",
       " ('7382347c23fd5d84ec6000c19c68576b5277a313', 'internetstandards/Internet.nl'),\n",
       " ('73f7c6ced12ae7564f22d134a6abce3410db4f99', 'seriaati/hoyo-buddy'),\n",
       " ('76929b6164e32cb92cc90ea8ceaf379349a67092', 'Belphemur/SoundSwitch'),\n",
       " ('78ba807e38f66264ab456ed0cb424bd526791956', 'FireDiscordBot/bot'),\n",
       " ('7e213297fa4310e778620d0d312a38acf53ca1a8', 'internetstandards/Internet.nl'),\n",
       " ('7e4590488a09d7b7dfbfbfe2b977497bb6919d6f', 'mano-sesan/mano'),\n",
       " ('7fdaa383f592600862ca89277d8b4bbe2dfe2b24', 'SkyTemple/skytemple-rust'),\n",
       " ('80831e27d488f6cc4f9209c9806b68401e17375b', 'getsentry/sentry'),\n",
       " ('827f4c7319ddd687b3bbed6cd5678c325d6f6455', 'getsentry/sentry'),\n",
       " ('863b5a0bc25e36c2038ab51c05c6bc338e11a925', 'seriaati/hoyo-buddy'),\n",
       " ('8859fafb649180d85016685bd1f4126a46be631a', 'PostHog/posthog'),\n",
       " ('8859fafb649180d85016685bd1f4126a46be631a', 'PostHog/posthog-foss'),\n",
       " ('88a3ddf9d04c947b54a47b570989e826f82e08a1', 'getsentry/sentry'),\n",
       " ('89626086392ffa16bca5c37105cf9de1cce8b246', 'FireDiscordBot/bot'),\n",
       " ('8b748929dc6d3c621be555be051ad38137389195', 'getsentry/sentry'),\n",
       " ('8cc4e0d3c6e7d9cc8c77818300ce0fe747cc6a6f', 'getsentry/sentry'),\n",
       " ('8d3daa03a346756770d66e1a031bf3b18ceb3f22', 'getsentry/sentry'),\n",
       " ('9120de7fcf8362dbf4d09fd2fcba34ffee13cf43', 'TabbycatDebate/tabbycat'),\n",
       " ('9233da610b136c0e6654ff2b307aa8aa25db14d2', 'getsentry/sentry'),\n",
       " ('929b3295880d1d133f95abbcc4f197981448be9a', 'YodaBotOS/YodaBot'),\n",
       " ('95b68cbac3132e2f9533153ad4e0b3e23a8f21f1', 'getsentry/sentry'),\n",
       " ('9663942c56fa0cb36da74fc913b969468d8bc13c', 'GorenPnP/pnpWebsite'),\n",
       " ('966c9b0e7c83cddf5d0942122881c54bffc00e3e', 'getsentry/sentry'),\n",
       " ('985830407f0491fb035fa443a1fad068f95b0b11', 'seriaati/hoyo-buddy'),\n",
       " ('9a5da6ed1dae902cb568fe4cea13579d645fda9d', 'getsentry/sentry'),\n",
       " ('9a68487598fc4924f4cc7b589ccf72a9b612ca02', 'bfkeinberg/route-forecast'),\n",
       " ('9fef046b7968db5fe82a21ebbce640c4d3928e45', 'keymanapp/keyman'),\n",
       " ('a0036a9428f50f7f2b34ddc4ce4942724e8f49ab', 'liberapay/liberapay.com'),\n",
       " ('a0b08299c5bb65f765f6e2cf125d5a528e9b0d03', 'seriaati/hoyo-buddy'),\n",
       " ('a13baf53d17659d12326a16692487ffaa2894807', 'getsentry/sentry'),\n",
       " ('a50c6416dd23cf0bd4625a518b81a24a71ccabe9', 'GorenPnP/pnpWebsite'),\n",
       " ('a7323e147381d550964f47c437b36391949c65e0', 'freelawproject/courtlistener'),\n",
       " ('a742f50066dfe1019caa842f2a5ae32e3c5d8cc5', 'getsentry/sentry'),\n",
       " ('a79af6e9386e5cb7edcffa42afa6449c1810e81d', 'getsentry/sentry'),\n",
       " ('a85de1f580d105e5b1ddf99710d2930b87a9e095', 'getsentry/sentry'),\n",
       " ('ad66ec0066d02bfce32a144b44b35eab1caf62e9', 'Belphemur/SoundSwitch'),\n",
       " ('aedf114633fb08125a55142c72ea2a6aeb5421a8', 'seriaati/hoyo-buddy'),\n",
       " ('b0941e1d2141175bcfca30206cdfb1094cc9eaf4', 'elamperti/OpenWebScrobbler'),\n",
       " ('b31c7c03d11b9c29c80b08bf27e4728a7214800c', 'seriaati/hoyo-buddy'),\n",
       " ('bb0b993803619ae28a0c1ee094c5c0b4a7f3a0ec', 'getsentry/sentry'),\n",
       " ('bbcae3511e7fa55c5a674cf57a4763a77a60bffb', 'freelawproject/courtlistener'),\n",
       " ('bd4791e883e3961aeaae3b4e707dc7d1051aaa38', 'getsentry/sentry'),\n",
       " ('bd51528ea4e23f22c5f96b59183d2afe323e15c3', 'Belphemur/SoundSwitch'),\n",
       " ('bf41dfe2b58c7b6f8e10889ed22e2683689fd920', 'rustymotors/server'),\n",
       " ('bf6b6eab692965373da33ffda722143cb5c9bb52', 'freelawproject/courtlistener'),\n",
       " ('c00077c5d8fa77a50b340e028bb7023146699e25',\n",
       "  'lafranceinsoumise/actionpopulaire.fr'),\n",
       " ('c0ab49dbc780363d53083a73908ac0f65dc4af43', 'TabbycatDebate/tabbycat'),\n",
       " ('c15534e781e2451131d81b98d69da456496181dd', 'getsentry/sentry'),\n",
       " ('c173b0898b0cd7bcf079a95835d1fa0886fada58', 'bfkeinberg/route-forecast'),\n",
       " ('c41eead927a4d32e11405cc79af0315d22531d82',\n",
       "  'lafranceinsoumise/actionpopulaire.fr'),\n",
       " ('c839bc89eb56da200a1329b9eb8dabb98a3d08c1', 'flyinghead/flycast'),\n",
       " ('c850aa536591921d4a9f443515e352da55aaca23', 'Ultimaker/Uranium'),\n",
       " ('c897df2a6c88b4447ce74e30d8897f25d43d6865', 'throneteki/throneteki'),\n",
       " ('c921dc3b6d82864a907cb85781e291243950ed94', 'freelawproject/courtlistener'),\n",
       " ('c9ee7bf87c620ecd5165ca65dfdf4008cace3c07', 'getsentry/sentry'),\n",
       " ('ccfc6a924a805268b33ca68e23df213fa18f0a58', 'liberapay/liberapay.com'),\n",
       " ('cd1709d9e5a10334e75b4a52022219aaadb74b07', 'FireDiscordBot/bot'),\n",
       " ('d16d746d0370a18b369aab9f867ae861f13ed38d', 'seriaati/hoyo-buddy'),\n",
       " ('d1a3a168b4395d858f4c5d8277faf097f3b858f9', 'getsentry/sentry'),\n",
       " ('d254a40825a832475da027972941e6743e22ae50', 'TabbycatDebate/tabbycat'),\n",
       " ('d30ffa0884a63e4aa0a72628acede9a133382d1c', 'seriaati/hoyo-buddy'),\n",
       " ('d387cb321ca7899db242da7790d1c79e36188e10',\n",
       "  'lafranceinsoumise/actionpopulaire.fr'),\n",
       " ('d5640243acc42b2444fb5481f0a27e43e6fb6e90', 'bfkeinberg/route-forecast'),\n",
       " ('d5d74a340ffff53bd8eac8f9912a831c5f5551ff', 'getsentry/sentry'),\n",
       " ('de6bd39127a201cc24ad0ffafb70907e8729cf7a', 'getsentry/sentry'),\n",
       " ('de6c3afad3e8eb67fe0f4cccc389f4374a87fea0', 'getsentry/sentry'),\n",
       " ('e12a83c8048c2a7b349eedcd3ecc315a853baa2b', 'getsentry/sentry'),\n",
       " ('e13c5c99ebc2289a7b90d766aad9401451ec36c0', 'internetstandards/Internet.nl'),\n",
       " ('e1457f25ce062f8144e2920eac834f54fc48e960', 'getsentry/sentry'),\n",
       " ('e1b4d4d703102545e33415604771cbe216c2e08e', 'flyinghead/flycast'),\n",
       " ('e21d54f02597960f902104ffc6bbbc65feee5f7e', 'seriaati/hoyo-buddy'),\n",
       " ('e262e311e3bea156028453faf60481c533ce700c', 'argos-ci/argos'),\n",
       " ('e2d4e647c198417a72c4e0d94685cc5e1321a727', 'getsentry/sentry'),\n",
       " ('e31ec44c084b2d34b67d5320216fecb32daeeb8d', 'liberapay/liberapay.com'),\n",
       " ('e7171a9ceb7ac3fdf76e391d8b76c7b5c8f2a310', 'seriaati/hoyo-buddy'),\n",
       " ('e73a08024353ee7a95f34f8d0041a2772f89da61', 'akhilnarang/vlrgg-scraper'),\n",
       " ('e88984dc85ec2b3fd722cfe48019dcfd39076863', 'getsentry/sentry'),\n",
       " ('e9a32efbd8d663c46979a848832b2bd4056626cc', 'getsentry/sentry'),\n",
       " ('e9d67788de262aee13a985753d93387cd9488a95', 'getsentry/sentry'),\n",
       " ('ecb75f9f9d6b44c203ad688de76e1ab9e0f19656', 'getsentry/sentry'),\n",
       " ('ef9811f20d60ab23e90ec5cc46387b667d01e043', 'seriaati/hoyo-buddy'),\n",
       " ('f45305f7e7b962ab43a1b09360f41ea2feed05da', 'getsentry/sentry'),\n",
       " ('f4d2737e4590be8792dc07d14711959e1f11f0e6', 'getsentry/sentry'),\n",
       " ('f6cffae0e62ba27128c6253e7ca34a0d6538d906', 'getsentry/sentry'),\n",
       " ('f797f929eeb48defb91ecb709ee78a4e677f20e1', 'seriaati/hoyo-buddy'),\n",
       " ('f8c5a91209479b7916cd29e5979a8492b8e9ba1d', 'getsentry/sentry'),\n",
       " ('fa139fe5ba6c95eb40f5108424d7e0391ce6d902', 'getsentry/sentry'),\n",
       " ('faf254f7f6fd7f7eba494d6220514cffbe919b8b', 'Ultimaker/Uranium'),\n",
       " ('fbd22f8aece349c4cb57a36c7663b5b0de6a544c', 'TabbycatDebate/tabbycat'),\n",
       " ('fc5d9f1db6ffa26223a9b7b977cee54aef59a30b', 'Ultimaker/Uranium'),\n",
       " ('fd2e67836ea924977b3d84f06b8251fb3effcb42', 'getsentry/sentry')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_repos(filenames):\n",
    "    items = []\n",
    "    for filename in filenames:\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "            for i, cur in tqdm(enum(data), desc='Validating Data'):\n",
    "                item = EvalItemWithDiff.model_validate(cur)\n",
    "                items.append((item.commit_hash, item.repo_name))\n",
    "    return set(items)\n",
    "    \n",
    "\n",
    "get_repos(['../data/eval_os_items_from_db_apr_23_90_days.json', '../data/eval_sentry_items_from_db_apr_23_90_days.json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_repos('../data/eval_os_items_from_db_apr_23_90_days.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
