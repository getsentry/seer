{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create a dataset from Sentry Issues <-> Github commits that reference a sentry issue and save it to langsmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the github client and instantiate the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from github import Github\n",
    "from github.Auth import Token\n",
    "\n",
    "github = Github(auth=Token(token=os.environ.get('GITHUB_TOKEN')))\n",
    "repo = github.get_repo('getsentry/sentry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def get_resolved_issues(organization_slug=\"sentry\", project_slug=\"sentry\", cursor=None):\n",
    "    url = f\"https://sentry.io/api/0/projects/{organization_slug}/{project_slug}/issues/?query=is:resolved error.type:TypeError\"\n",
    "\n",
    "    if cursor: \n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {os.environ.get('SENTRY_AUTH_TOKEN')}\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    result = response.json()\n",
    "\n",
    "    if \"detail\" in result:\n",
    "        raise Exception(result[\"detail\"])\n",
    "\n",
    "    return result, response.links[\"next\"]\n",
    "\n",
    "\n",
    "def get_issue_by_id(issue_id, organization_slug=\"sentry\"):\n",
    "    url = (\n",
    "        f\"https://sentry.io/api/0/organizations/{organization_slug}/issues/{issue_id}/\"\n",
    "    )\n",
    "    headers = {\"Authorization\": f\"Bearer {os.environ.get('SENTRY_AUTH_TOKEN')}\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    issue = response.json()\n",
    "\n",
    "    if \"detail\" in issue and issue[\"detail\"] == \"The requested resource does not exist\":\n",
    "        raise Exception(f\"Could not find issue with id {issue_id}\")\n",
    "\n",
    "    return issue\n",
    "\n",
    "\n",
    "def get_issue_id_from_short_id(short_id, organization_slug=\"sentry\"):\n",
    "    url = f\"https://sentry.io/api/0/organizations/{organization_slug}/shortids/{short_id}/\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.environ.get('SENTRY_AUTH_TOKEN')}\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    result = response.json()\n",
    "\n",
    "    if ( \n",
    "        \"detail\" in result\n",
    "        and result[\"detail\"] == \"The requested resource does not exist\"\n",
    "    ):\n",
    "        raise Exception(f\"Could not find issue with short id {short_id}\")\n",
    "\n",
    "    return result[\"groupId\"]\n",
    "\n",
    "\n",
    "def get_details_for_issue(issue_id=None, short_id=None, organization_slug=\"sentry\"):\n",
    "    if issue_id is None and short_id is None:\n",
    "        raise Exception(\"Either issue_id or short_id must be provided\")\n",
    "    if short_id:\n",
    "        issue_id = get_issue_id_from_short_id(short_id, organization_slug)\n",
    "\n",
    "    issue = get_issue_by_id(issue_id, organization_slug)\n",
    "\n",
    "    url = f\"https://sentry.io/api/0/organizations/{organization_slug}/issues/{issue['id']}/events/?full=true\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.environ.get('SENTRY_AUTH_TOKEN')}\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    events = response.json()\n",
    "\n",
    "    return dict(\n",
    "        **issue,\n",
    "        events=events[:1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_serializer, BaseModel\n",
    "from github.Commit import Commit\n",
    "from typing import Any\n",
    "from pydantic import ConfigDict, field_validator\n",
    "\n",
    "from seer.automation.autofix.models import IssueDetails, EventDetails\n",
    "\n",
    "class EvalItem(BaseModel):\n",
    "    raw_data: dict[str, Any]\n",
    "    commit: Commit\n",
    "    issue: IssueDetails\n",
    "    event: EventDetails\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "\n",
    "    @field_serializer('commit')\n",
    "    def serialize_commit(self, commit: Commit, _info):\n",
    "        return commit.sha\n",
    "    \n",
    "    @field_validator('commit', mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_commit(cls, commit: Commit | str):\n",
    "        return commit if isinstance(commit, Commit) else repo.get_commit(commit)\n",
    "    \n",
    "class EvalItemWithDiff(EvalItem):\n",
    "    diff: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the commits for a timeframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import datetime\n",
    "\n",
    "days_ago = datetime.datetime.now() - timedelta(days=90)\n",
    "commits = repo.get_commits(since=days_ago)\n",
    "all_commits = [commit for commit in commits]\n",
    "print('Total commits in this timeframe: ', commits.totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter it down to only commits with sentry issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the commits with an id or url to a sentry issue\n",
    "import re\n",
    "\n",
    "has_sentry_issue_linked = []\n",
    "\n",
    "for commit in all_commits:\n",
    "    if 'SENTRY-' in commit.commit.message or 'https://sentry.sentry.io/issues/' in commit.commit.message:\n",
    "        has_sentry_issue_linked.append(commit)\n",
    "\n",
    "# Extracts the short id or id from the commit message\n",
    "\n",
    "with_id_or_url = []\n",
    "\n",
    "for commit in has_sentry_issue_linked:\n",
    "    message = commit.commit.message\n",
    "    issue_short_id_match = re.findall(r'SENTRY-.{4}', message)\n",
    "    issue_short_id = issue_short_id_match[0] if issue_short_id_match else None\n",
    "    issue_url = re.findall(r'https://sentry.sentry.io/issues/\\d+', message)\n",
    "    issue_id = issue_url[0].split('/')[-1] if issue_url else None\n",
    "\n",
    "    if issue_short_id or issue_id:\n",
    "        with_id_or_url.append((commit, issue_short_id, issue_id))\n",
    "\n",
    "print('Commits with sentry issue id or url:', len(with_id_or_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate into eval items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_items: list[EvalItem] = []\n",
    "skipped_items: list[EvalItem] = []\n",
    "error_count = 0\n",
    "\n",
    "with tqdm(total=len(with_id_or_url)) as pbar: \n",
    "    for commit, short_id, issue_id in with_id_or_url:\n",
    "        try:\n",
    "            issue = get_details_for_issue(issue_id=issue_id, short_id=short_id)\n",
    "            issue_details = IssueDetails.model_validate(issue)\n",
    "            event_details = EventDetails.from_event(issue_details.events[0])\n",
    "\n",
    "            eval_item = EvalItem(\n",
    "                commit=commit,\n",
    "                raw_data=issue,\n",
    "                issue=issue_details,\n",
    "                event=event_details\n",
    "            )\n",
    "\n",
    "            if len(event_details.exceptions) == 0:\n",
    "                skipped_items.append(eval_item)\n",
    "                continue\n",
    "\n",
    "            eval_items.append(eval_item)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing commit: {e}')\n",
    "            error_count += 1\n",
    "        finally:\n",
    "            pbar.update(1)\n",
    "        \n",
    "\n",
    "print('Total eval items:', len(eval_items))\n",
    "print('Total skipped items:', len(skipped_items))\n",
    "print('Total errors:', error_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt GPT so we can filter it down to only issues that are \"actionable\" which means in this case, given a sentry issue, it should be evident what the developer should do to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from github.Commit import Commit\n",
    "from github.File import File\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
    "\n",
    "\n",
    "def file_patch_to_str(file: File):\n",
    "    return f\"[{file.filename}]\\n{file.patch}\"\n",
    "\n",
    "\n",
    "def explain_changes(error_msg, stack_str, commit_message, files_str):\n",
    "    response = model.invoke(\n",
    "        f\"\"\"<error_message>\n",
    "{error_msg}\n",
    "</error_message>\n",
    "<stacktrace>\n",
    "{stack_str}\n",
    "</stacktrace>\n",
    "\n",
    "A software engineer then created the following changes in a commit to fix the above issue:\n",
    "<commit_message>\n",
    "{commit_message}\n",
    "</commit_message>\n",
    "<changes>\n",
    "{files_str}\n",
    "</changes>\n",
    "\n",
    "How would you describe the solution to the error in a short summary. Also describe what the root cause of the problem ended up being.\"\"\"\n",
    "    )\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def determine_actionability(error_msg, stack_str, solution):\n",
    "    response = model.invoke(\n",
    "        f\"\"\"Given the provided information:\n",
    "<information>\n",
    "<error_message>\n",
    "{error_msg}\n",
    "</error_message>\n",
    "<stacktrace>\n",
    "{stack_str}\n",
    "</stacktrace>\n",
    "</information>\n",
    "\n",
    "<expected_solution>\n",
    "{solution}\n",
    "</expected_solution>\n",
    "\n",
    "Based on the error message and stacktrace, can the solution be inferred from the information given and access to reading the codebase? Why or why not?\n",
    "Answer in the format:<response>yes/no</response><reason>reason for the response</reason>\"\"\"\n",
    "    )\n",
    "    comatch = re.match(r\"<response>(.*?)</response>\", response.content)\n",
    "    if comatch and \"yes\" in comatch.group(1).lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "fixable_items: list[EvalItem] = []\n",
    "panel_n = 5\n",
    "with tqdm(total=len(eval_items)) as pbar:\n",
    "    for eval_item in eval_items:\n",
    "        stacktrace = eval_item.event.exceptions[0].stacktrace\n",
    "\n",
    "        stacktrace_str = stacktrace.to_str(max_frames=64)\n",
    "\n",
    "        commit = eval_item.commit\n",
    "        files = commit.files\n",
    "        files_str = \"\\n\".join([file_patch_to_str(file) for file in files])\n",
    "\n",
    "        explain_result = explain_changes(\n",
    "            issue_details.title, stacktrace_str, commit.commit.message, files_str\n",
    "        )\n",
    "\n",
    "        actionability_results = []\n",
    "        final_result = False\n",
    "        for _ in range(panel_n):\n",
    "            actionability_result = determine_actionability(issue_details.title, stacktrace_str, explain_result)\n",
    "            actionability_results.append(actionability_result)\n",
    "            true_count = actionability_results.count(True)\n",
    "            false_count = actionability_results.count(False)\n",
    "            if true_count > panel_n / 2:\n",
    "                final_result = True\n",
    "                break\n",
    "            if false_count > panel_n / 2:\n",
    "                final_result = False\n",
    "                break\n",
    "        print(f\"Issue: '{eval_item.issue.title}': \", final_result)\n",
    "        \n",
    "        if final_result:\n",
    "            fixable_items.append(eval_item)\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "print('Total fixable items:', len(fixable_items))\n",
    "print('Total non-fixable items:', len(eval_items) - len(fixable_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the eval items that are fixable with their expected diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_items: list[EvalItemWithDiff] = []\n",
    "for item in fixable_items:\n",
    "    comparison = repo.compare(item.commit.commit.parents[0].sha, item.commit.sha)\n",
    "    \n",
    "    requester = repo._requester\n",
    "    headers = {\n",
    "        \"Authorization\": f\"{requester._Requester__auth.token_type} {requester._Requester__auth.token}\",  # type: ignore\n",
    "        \"User-Agent\": requester._Requester__userAgent,  # type: ignore\n",
    "    }\n",
    "    diff_data = requests.get(comparison.diff_url, headers=headers).content.decode('utf-8')\n",
    "\n",
    "    final_item = EvalItemWithDiff.model_validate(dict(\n",
    "        **dict(item),\n",
    "        diff=diff_data\n",
    "    ))\n",
    "    final_eval_items.append(final_item)\n",
    "\n",
    "print('Total final eval items:', len(final_eval_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the dataset to a json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dump_items(items: list[EvalItem], filename: str):\n",
    "    serialized_items = [item.model_dump(mode='json') for item in items]\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(serialized_items, f)\n",
    "\n",
    "dump_items(eval_items, '../data/eval_items.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset to langsmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "dataset_inputs = [item.model_dump(mode='json') for item in final_eval_items]\n",
    "\n",
    "dataset_outputs = [{ \"diff\": item.diff } for item in final_eval_items]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Autofix Eval Full 240314\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Autofix full eval made from mapping sentry <-> github commits for sentry project\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=dataset_inputs,\n",
    "    outputs=dataset_outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
