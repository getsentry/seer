{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from github import Github\n",
    "from github.Auth import Token\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "github = Github(auth=Token(token=os.environ.get('GITHUB_TOKEN')))\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('autofix')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.handlers = []\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "import os\n",
    "os.environ['DATABASE_URL'] = \"postgresql+psycopg://root:seer@localhost:5433/seer\"\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"ai-autofix-evals\"\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['CODEBASE_STORAGE_TYPE'] = 'filesystem'\n",
    "os.environ['CODEBASE_WORKSPACE_DIR'] = '../data/chroma/workspaces'\n",
    "os.environ['CODEBASE_STORAGE_DIR'] = '../data/chroma/storage'\n",
    "\n",
    "from seer.bootup import bootup\n",
    "\n",
    "bootup(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_serializer, BaseModel\n",
    "from github.Commit import Commit\n",
    "from typing import Any, Optional\n",
    "from pydantic import ConfigDict, field_validator\n",
    "\n",
    "from seer.automation.models import IssueDetails, EventDetails\n",
    "\n",
    "class EvalItem(BaseModel):\n",
    "    raw_data: dict[str, Any]\n",
    "    organization_id: int\n",
    "    project_id: int\n",
    "    repo_name: Optional[str] = None\n",
    "    commit_hash: Optional[str] = None\n",
    "    # Field order matters as commit is dependent on repo_name and commit_hash, it should come later down the order.\n",
    "    commit: Commit | str\n",
    "    issue: IssueDetails\n",
    "    event: EventDetails\n",
    "    \n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "    \n",
    "    @field_serializer('commit')\n",
    "    def serialize_commit(self, commit: Commit, _info):\n",
    "        return commit.sha\n",
    "    \n",
    "    @field_validator('commit', mode=\"after\")\n",
    "    @classmethod\n",
    "    def validate_commit(cls, commit: Commit | str, values, **kwargs):\n",
    "        if isinstance(commit, Commit):\n",
    "            return commit\n",
    "        if 'repo_name' in values.data and values.data['repo_name'] is not None :\n",
    "            repo_name = values.data['repo_name']\n",
    "        else:\n",
    "            repo_name = 'getsentry/sentry'\n",
    "            values.data['repo_name'] = repo_name\n",
    "        repo = github.get_repo(repo_name)\n",
    "        values.data['commit_hash'] = commit\n",
    "        return repo.get_commit(commit)\n",
    "        \n",
    "class EvalItemWithDiff(EvalItem):\n",
    "    diff: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain.smith import RunEvalConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create codebase indexes for each sha in the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seer.automation.autofix.models import RepoDefinition\n",
    "from seer.automation.codebase.codebase_index import CodebaseIndex\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "langsmith_client = Client()\n",
    "dataset_name = \"Autofix Eval 100 240423\"\n",
    "\n",
    "examples = langsmith_client.list_examples(dataset_name=dataset_name)\n",
    "dataset = langsmith_client.read_dataset(dataset_name=dataset_name)\n",
    "embedding_model = SentenceTransformer(\"../models/autofix_embeddings_v0\", trust_remote_code=True)\n",
    "embedding_model.max_seq_length = 4096\n",
    "embedding_model.to(device = torch.device(get_device()))\n",
    "successful = []\n",
    "skipped = []\n",
    "errored = []\n",
    "\n",
    "with (tqdm(examples, total=dataset.example_count, position=0, leave=True) as pbar,\n",
    "    tqdm(desc=\"Skipped (pre-existing)\", position=1, leave=True) as ctr1, tqdm(desc=\"Skipped (error)\", position=2, leave=True) as ctr2):\n",
    "    for example in pbar:\n",
    "        eval_item = EvalItem.model_validate(example.inputs)\n",
    "        if eval_item.repo_name == 'inikoo/aurora':\n",
    "            print(f'Skipping inikoo/aurora as it takes too long')\n",
    "            errored.append({'repo':eval_item.repo_name, 'error':'chunking gets stuck at 17%'})\n",
    "            ctr2.update(1)\n",
    "            continue\n",
    "            \n",
    "        [repo_owner, repo_name] = eval_item.repo_name.split('/')\n",
    "        pbar.set_description(eval_item.repo_name)\n",
    "\n",
    "\n",
    "        repo_definition = RepoDefinition(provider=\"github\", owner=repo_owner, name=repo_name, external_id=eval_item.repo_name)\n",
    "        try:\n",
    "            if CodebaseIndex.has_repo_been_indexed(\n",
    "                organization=eval_item.organization_id, \n",
    "                project=eval_item.project_id, \n",
    "                repo=repo_definition,\n",
    "                sha=eval_item.commit.parents[0].sha):\n",
    "                ctr1.update(1)\n",
    "                skipped.append(eval_item.repo_name)\n",
    "            else:\n",
    "                codebase = CodebaseIndex.create(\n",
    "                    organization=eval_item.organization_id, project=eval_item.project_id, \n",
    "                    repo=repo_definition,\n",
    "                    embedding_model=embedding_model, \n",
    "                    sha=eval_item.commit.parents[0].sha)\n",
    "                successful.append(eval_item.repo_name)\n",
    "                codebase.cleanup()\n",
    "        except Exception as e:\n",
    "            errored.append({'repo':eval_item.repo_name, type(e):e})\n",
    "            ctr2.update(1)\n",
    "            \n",
    "if len(errored) > 0:\n",
    "    print('----------------Errors-------------')\n",
    "    for err in errored:\n",
    "        print(err, '-----------------------')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the examples for which repo indexing failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errored_repos = set([err['repo'] for err in errored])\n",
    "examples = langsmith_client.list_examples(dataset_name=dataset_name)\n",
    "to_delete = []\n",
    "for example in examples:\n",
    "    if example.inputs['repo_name'] in errored_repos:\n",
    "        to_delete.append(example.id)\n",
    "print(to_delete)\n",
    "\n",
    "for cur_del in to_delete:\n",
    "    langsmith_client.delete_example(example_id=cur_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the db repos and namespaces into jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from seer.automation.codebase.models import CodebaseNamespace, RepositoryInfo\n",
    "from seer.db import DbCodebaseNamespace, DbRepositoryInfo, Session\n",
    "\n",
    "def get_namespace_dumps():\n",
    "    with Session() as session:\n",
    "        repository_info = session.query(DbRepositoryInfo).all()\n",
    "        codebase_namespaces = session.query(DbCodebaseNamespace).all()\n",
    "        \n",
    "        repo_infos = [RepositoryInfo.from_db(repo_info).model_dump_json() for repo_info in repository_info]\n",
    "        namespaces = [CodebaseNamespace.from_db(codebase_namespace).model_dump_json() for codebase_namespace in codebase_namespaces]\n",
    "\n",
    "    return repo_infos, namespaces\n",
    "\n",
    "repo_infos, namespaces = get_namespace_dumps()\n",
    "\n",
    "with open('data/repo_infos.json', 'w') as f:\n",
    "    f.write(json.dumps(repo_infos))\n",
    "\n",
    "with open('data/namespaces.json', 'w') as f:\n",
    "    f.write(json.dumps(namespaces))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from seer.automation.codebase.models import CodebaseNamespace, RepositoryInfo\n",
    "from seer.db import DbCodebaseNamespace, DbRepositoryInfo, Session\n",
    "\n",
    "with Session() as session:\n",
    "    repository_info = session.query(DbRepositoryInfo).all()\n",
    "    print('---------- Repos -------------')\n",
    "    for info in repository_info:\n",
    "        print(RepositoryInfo.from_db(info).model_dump_json())\n",
    "    print('---------- Namespaces -------------')\n",
    "    namespace_info = session.query(DbCodebaseNamespace).all()\n",
    "    for info in namespace_info:\n",
    "        print(CodebaseNamespace.from_db(info).model_dump_json())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Load the repos and codebase namespaces into your postgres. This assumes your postgres is clean because the ids will probably conflict otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from seer.automation.codebase.models import RepositoryInfo, CodebaseNamespace\n",
    "from seer.db import Session\n",
    "\n",
    "def load_json_into_db(repo_info_file='../data/repo_infos.json', namespace_file='../data/namespaces.json'):\n",
    "    with open(repo_info_file, 'r') as f:\n",
    "        repo_infos_json = json.load(f)\n",
    "    \n",
    "    with open(namespace_file, 'r') as f:\n",
    "        namespaces_json = json.load(f)\n",
    "    \n",
    "    with Session() as session:\n",
    "        for repo_info in repo_infos_json:\n",
    "            db_repo_info = RepositoryInfo.model_validate_json(repo_info).to_db_model()\n",
    "            session.merge(db_repo_info)\n",
    "\n",
    "        session.flush()\n",
    "        \n",
    "        for namespace in namespaces_json:\n",
    "            db_namespace = CodebaseNamespace.model_validate_json(namespace).to_db_model()\n",
    "            session.merge(db_namespace)\n",
    "        \n",
    "        session.commit()\n",
    "\n",
    "load_json_into_db()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
