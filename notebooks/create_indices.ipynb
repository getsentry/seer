{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennmueng/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/tqdm-4.66.1-py3.11.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Flask '__main__'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from github import Github\n",
    "from github.Auth import Token\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "github = Github(auth=Token(token=os.environ.get('GITHUB_TOKEN')))\n",
    "repo = github.get_repo('getsentry/sentry')\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('autofix')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.handlers = []\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "import os\n",
    "os.environ['DATABASE_URL'] = \"postgresql+psycopg://root:seer@localhost:5433/seer\"\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"ai-autofix-evals\"\n",
    "\n",
    "from seer.bootup import bootup\n",
    "\n",
    "bootup(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_serializer, BaseModel\n",
    "from github.Commit import Commit\n",
    "from typing import Any\n",
    "from pydantic import ConfigDict, field_validator\n",
    "\n",
    "from seer.automation.autofix.models import IssueDetails, EventDetails\n",
    "\n",
    "class EvalItem(BaseModel):\n",
    "    raw_data: dict[str, Any]\n",
    "    commit: Commit\n",
    "    issue: IssueDetails\n",
    "    event: EventDetails\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "\n",
    "    @field_serializer('commit')\n",
    "    def serialize_commit(self, commit: Commit, _info):\n",
    "        return commit.sha\n",
    "    \n",
    "    @field_validator('commit', mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_commit(cls, commit: Commit | str):\n",
    "        return commit if isinstance(commit, Commit) else repo.get_commit(commit)\n",
    "    \n",
    "class EvalItemWithDiff(EvalItem):\n",
    "    diff: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "\n",
    "langsmith_client = Client()\n",
    "dataset_name = \"Autofix Eval Full 240314\"\n",
    "\n",
    "examples = langsmith_client.list_examples(dataset_name=dataset_name)\n",
    "dataset = langsmith_client.read_dataset(dataset_name=dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create codebase indexes for each sha in the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seer.automation.autofix.models import RepoDefinition\n",
    "from seer.automation.codebase.codebase_index import CodebaseIndex\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "repo_definition = RepoDefinition(provider=\"github\", owner=\"getsentry\", name=\"sentry\")\n",
    "\n",
    "embedding_model = SentenceTransformer(\"../models/autofix_embeddings_v0\", trust_remote_code=True)\n",
    "embedding_model.max_seq_length = 4096\n",
    "\n",
    "with tqdm(total=dataset.example_count) as pbar:\n",
    "    for example in examples:\n",
    "        eval_item = EvalItem.model_validate(example.inputs)\n",
    "        try:\n",
    "            codebase = CodebaseIndex.create(1, 1, repo_definition, uuid.uuid4(), embedding_model=embedding_model, sha=eval_item.commit.parents[0].sha)\n",
    "            codebase.cleanup()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create codebase for {eval_item.commit.sha}: {e}\")\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the db repos and namespaces into jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from seer.automation.codebase.models import CodebaseNamespace, RepositoryInfo\n",
    "from seer.db import DbCodebaseNamespace, DbRepositoryInfo, Session\n",
    "\n",
    "def get_namespace_dumps():\n",
    "    with Session() as session:\n",
    "        repository_info = session.query(DbRepositoryInfo).all()\n",
    "        codebase_namespaces = session.query(DbCodebaseNamespace).all()\n",
    "        \n",
    "        repo_infos = [RepositoryInfo.from_db(repo_info).model_dump_json() for repo_info in repository_info]\n",
    "        namespaces = [CodebaseNamespace.from_db(codebase_namespace).model_dump_json() for codebase_namespace in codebase_namespaces]\n",
    "\n",
    "    return repo_infos, namespaces\n",
    "\n",
    "repo_infos, namespaces = get_namespace_dumps()\n",
    "\n",
    "with open('../data/repo_infos.json', 'w') as f:\n",
    "    f.write(json.dumps(repo_infos))\n",
    "\n",
    "with open('../data/namespaces.json', 'w') as f:\n",
    "    f.write(json.dumps(namespaces))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the repos and codebase namespaces into your postgres. This assumes your postgres is clean because the ids will probably conflict otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from seer.automation.codebase.models import RepositoryInfo, CodebaseNamespace\n",
    "from seer.db import Session\n",
    "\n",
    "def load_json_into_db(repo_info_file='../data/repo_infos.json', namespace_file='../data/namespaces.json'):\n",
    "    with open(repo_info_file, 'r') as f:\n",
    "        repo_infos_json = json.load(f)\n",
    "    \n",
    "    with open(namespace_file, 'r') as f:\n",
    "        namespaces_json = json.load(f)\n",
    "    \n",
    "    with Session() as session:\n",
    "        for repo_info in repo_infos_json:\n",
    "            db_repo_info = RepositoryInfo.model_validate_json(repo_info).to_db_model()\n",
    "            session.merge(db_repo_info)\n",
    "\n",
    "        session.flush()\n",
    "        \n",
    "        for namespace in namespaces_json:\n",
    "            db_namespace = CodebaseNamespace.model_validate_json(namespace).to_db_model()\n",
    "            session.merge(db_namespace)\n",
    "        \n",
    "        session.commit()\n",
    "\n",
    "load_json_into_db()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
