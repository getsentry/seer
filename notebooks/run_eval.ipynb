{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autofix Evaluation\n",
    "This initial preliminary high-level evaluation for Autofix runs on a dataset of Sentry Issues <-> Github Commits.\n",
    "\n",
    "It is graded by a sending the expected diff vs the predicted diff to n GPTs with a prompt to evaluate whether the diff is a good fix or not.\n",
    "\n",
    "Returns the average score of the GPTs as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the seer requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple more libraries are needed for running the eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv 'psycopg[binary,pool]' langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DATABASE_URL'] = \"postgresql+psycopg://root:seer@localhost:5433/seer\"\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"ai-autofix-evals\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('autofix')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.handlers = []\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "from github import Github\n",
    "from github.Auth import Token\n",
    "\n",
    "github = Github(auth=Token(token=os.environ.get('GITHUB_TOKEN')))\n",
    "repo = github.get_repo('getsentry/sentry')\n",
    "\n",
    "from seer.bootup import bootup\n",
    "\n",
    "bootup(__name__)\n",
    "\n",
    "from langsmith import Client\n",
    "langsmith_client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_serializer, BaseModel\n",
    "from github.Commit import Commit\n",
    "from typing import Any\n",
    "from pydantic import ConfigDict, field_validator\n",
    "\n",
    "from seer.automation.autofix.models import IssueDetails, EventDetails\n",
    "\n",
    "class EvalItem(BaseModel):\n",
    "    raw_data: dict[str, Any]\n",
    "    commit: Commit\n",
    "    issue: IssueDetails\n",
    "    event: EventDetails\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "\n",
    "    @field_serializer('commit')\n",
    "    def serialize_commit(self, commit: Commit, _info):\n",
    "        return commit.sha\n",
    "    \n",
    "    @field_validator('commit', mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_commit(cls, commit: Commit | str):\n",
    "        return commit if isinstance(commit, Commit) else repo.get_commit(commit)\n",
    "    \n",
    "class EvalItemWithDiff(EvalItem):\n",
    "    diff: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predict function to be called during the eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seer.automation.autofix.autofix import Autofix\n",
    "from seer.automation.autofix.tasks import ContinuationState\n",
    "from seer.rpc import DummyRpcClient\n",
    "from seer.automation.autofix.models import (\n",
    "    AutofixContinuation,\n",
    "    AutofixRequest,\n",
    "    RepoDefinition,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from seer.automation.autofix.autofix_context import AutofixContext\n",
    "from seer.automation.autofix.event_manager import AutofixEventManager\n",
    "\n",
    "embedding_model = SentenceTransformer(\"../models/autofix_embeddings_v0\", trust_remote_code=True)\n",
    "embedding_model.max_seq_length = 4096\n",
    "\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    run_item = EvalItem.model_validate(input_)\n",
    "\n",
    "    # Initializes the rpc client in DRY RUN mode\n",
    "    rpc_client = DummyRpcClient()\n",
    "    rpc_client.dry_run = True\n",
    "\n",
    "    request = AutofixRequest(\n",
    "        organization_id=1,\n",
    "        project_id=1,\n",
    "        repos=[RepoDefinition(provider=\"github\", owner=\"getsentry\", name=\"sentry\")],\n",
    "        base_commit_sha=run_item.commit.parents[0].sha,\n",
    "        issue=run_item.issue,\n",
    "    )\n",
    "\n",
    "    state = ContinuationState(\n",
    "        val=AutofixContinuation(request=AutofixRequest.model_validate(request)), rpc_client=rpc_client\n",
    "    )\n",
    "\n",
    "    event_manager = AutofixEventManager(state)\n",
    "    context = AutofixContext(\n",
    "        organization_id=request.organization_id,\n",
    "        project_id=request.project_id,\n",
    "        repos=request.repos,\n",
    "        event_manager=event_manager,\n",
    "        state=state,\n",
    "        embedding_model=embedding_model,\n",
    "        sha=run_item.commit.parents[0].sha,\n",
    "    )\n",
    "    context.commit_changes = False\n",
    "    autofix = Autofix(context)\n",
    "\n",
    "    response = autofix.invoke(request)\n",
    "\n",
    "    if response is None:\n",
    "        return {\"output\": None}\n",
    "\n",
    "    return {\"output\": response['outputs'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the scoring prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from github.Commit import Commit\n",
    "from github.File import File\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "from seer.automation.autofix.models import AutofixOutput\n",
    "from seer.automation.autofix.prompts import format_exceptions\n",
    "from seer.automation.autofix.utils import extract_xml_element_text, escape_multi_xml\n",
    "\n",
    "n_panel = 3\n",
    "model = ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0.8)\n",
    "\n",
    "def score_fix_single_it(eval_item: EvalItemWithDiff, predicted_output: AutofixOutput) -> float:\n",
    "    completion = model.invoke(f\"\"\"<issue>\n",
    "<error_message>\n",
    "{eval_item.event.title}\n",
    "</error_message>\n",
    "<exceptions>\n",
    "{format_exceptions(eval_item.event.exceptions)}\n",
    "</exceptions>\n",
    "</issue>\n",
    "\n",
    "Given the above issue, we know the correct fix is:\n",
    "\n",
    "<expected_solution>\n",
    "<description>\n",
    "{eval_item.commit.commit.message}\n",
    "</description>\n",
    "<changes>\n",
    "{eval_item.diff}\n",
    "</changes>\n",
    "</expected_solution>\n",
    "\n",
    "The model outputted the following solution:\n",
    "\n",
    "<predicted_solution>\n",
    "{predicted_output.diff_str}\n",
    "</predicted_solution>\n",
    "\n",
    "Score how well the predicted solution matches the expected solution with a float score from 0 to 1, where 1 means the solution fully fixes the issue and 0 means the solution does not fix the issue at all.\n",
    "- Consider the context of the issue and the diff\n",
    "- Consider that there are multiple ways to fix an issue\n",
    "\n",
    "Think step-by-step inside a <thoughts> tag before giving a score.\n",
    "Return the score inside a <score> tag.\"\"\")\n",
    "    tree = ET.fromstring(f\"<root>{escape_multi_xml(completion.content, ['score'])}</root>\")\n",
    "    score_str = extract_xml_element_text(tree, 'score')\n",
    "    score = float(score_str) if score_str else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "@traceable(name=\"Score 1 item\", run_type=\"chain\")\n",
    "def score_one(eval_item: EvalItemWithDiff, predicted_output: AutofixOutput) -> float:\n",
    "    return round(sum([score_fix_single_it(eval_item, predicted_output) for _ in range(n_panel)]) / n_panel, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Autofix Eval Full 240314\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "@run_evaluator\n",
    "def gpt_panel(run: Run, example: Example | None = None):\n",
    "    eval_item = EvalItem.model_validate(run.inputs)\n",
    "    with_diff = EvalItemWithDiff.model_validate(dict(**dict(eval_item), diff=example.outputs.get('diff')))\n",
    "    score = score_one(with_diff, AutofixOutput.model_validate(run.outputs.get('output')))\n",
    "    return EvaluationResult(key=\"diff_gpt_panel_n3_score\", score=score)\n",
    "\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[gpt_panel]\n",
    ")\n",
    "\n",
    "ds = langsmith_client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "langsmith_client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=predict_result,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=\"Autofix v2 rev:4\",\n",
    "    concurrency_level=18\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test to just run one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = langsmith_client.list_examples(dataset_name=dataset_name)\n",
    "\n",
    "for example in examples:\n",
    "    predict_result(example.inputs)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
