{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autofix Evaluation\n",
    "This initial preliminary high-level evaluation for Autofix runs on a dataset of Sentry Issues <-> Github Commits.\n",
    "\n",
    "It is graded by a sending the expected diff vs the predicted diff to n GPTs with a prompt to evaluate whether the diff is a good fix or not.\n",
    "\n",
    "Returns the average score of the GPTs as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the seer requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple more libraries are needed for running the eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-dotenv 'psycopg[binary,pool]' langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pysqlite3-binary\n",
    "\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['DATABASE_URL'] = \"postgresql+psycopg://root:seer@localhost:5433/seer\"\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"ai-autofix-evals\"\n",
    "\n",
    "os.environ['CODEBASE_STORAGE_TYPE'] = 'filesystem'\n",
    "os.environ['CODEBASE_WORKSPACE_DIR'] = '../data/chroma/workspaces'\n",
    "os.environ['CODEBASE_STORAGE_DIR'] = '../data/chroma/storage'\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "import logging\n",
    "\n",
    "for logger_name in ['autofix']:\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers = []\n",
    "    logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "from github import Github\n",
    "from github.Auth import Token\n",
    "\n",
    "github = Github(auth=Token(token=os.environ.get('GITHUB_TOKEN')))\n",
    "# repo = github.get_repo('getsentry/sentry')\n",
    "\n",
    "from seer.bootup import bootup\n",
    "\n",
    "bootup(__name__)\n",
    "\n",
    "from langsmith import Client\n",
    "langsmith_client = Client()\n",
    "\n",
    "from seer.automation.autofix.pipelines import AutofixRootCause, AutofixExecution\n",
    "from seer.automation.autofix.tasks import ContinuationState\n",
    "from seer.rpc import DummyRpcClient\n",
    "from seer.automation.autofix.models import (\n",
    "    AutofixContinuation,\n",
    "    AutofixRequest,\n",
    "    ChangesStep,\n",
    "    RepoDefinition,\n",
    "    RootCauseStep,\n",
    "    SuggestedFixRootCauseSelection,\n",
    ")\n",
    "from pydantic import field_serializer, BaseModel\n",
    "from github.Commit import Commit\n",
    "from typing import Any, Optional\n",
    "from pydantic import ConfigDict, field_validator\n",
    "\n",
    "from seer.automation.models import IssueDetails, EventDetails\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from seer.automation.autofix.autofix_context import AutofixContext, AutofixCodebaseStateManager\n",
    "from seer.automation.autofix.event_manager import AutofixEventManager\n",
    "import numpy as np\n",
    "from seer.automation.codebase.codebase_index import CodebaseIndex\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dataset_name = \"Autofix Eval 100 240423\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalItem(BaseModel):\n",
    "    raw_data: dict[str, Any]\n",
    "    organization_id: int\n",
    "    project_id: int\n",
    "    repo_name: Optional[str] = None\n",
    "    commit_hash: Optional[str] = None\n",
    "    # Field order matters as commit is dependent on repo_name and commit_hash, it should come later down the order.\n",
    "    commit: Commit | str\n",
    "    issue: IssueDetails\n",
    "    event: EventDetails\n",
    "    \n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "    \n",
    "    @field_serializer('commit')\n",
    "    def serialize_commit(self, commit: Commit, _info):\n",
    "        return commit.sha\n",
    "    \n",
    "    @field_validator('commit', mode=\"after\")\n",
    "    @classmethod\n",
    "    def validate_commit(cls, commit: Commit | str, values, **kwargs):\n",
    "        if isinstance(commit, Commit):\n",
    "            return commit\n",
    "        if 'repo_name' in values.data and values.data['repo_name'] is not None :\n",
    "            repo_name = values.data['repo_name']\n",
    "        else:\n",
    "            repo_name = 'getsentry/sentry'\n",
    "            values.data['repo_name'] = repo_name\n",
    "        repo = github.get_repo(repo_name)\n",
    "        values.data['commit_hash'] = commit\n",
    "        return repo.get_commit(commit)\n",
    "        \n",
    "class EvalItemWithDiff(EvalItem):\n",
    "    diff: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check Embeddings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_embeddings():\n",
    "    examples = langsmith_client.list_examples(dataset_name=dataset_name)\n",
    "    summary = []\n",
    "    \n",
    "    for example in examples:\n",
    "        run_item = EvalItem.model_validate(example.inputs)\n",
    "        [repo_owner, repo_name] = run_item.repo_name.split('/')\n",
    "        request = AutofixRequest(\n",
    "            organization_id=run_item.organization_id,\n",
    "            project_id=run_item.project_id,\n",
    "            repos=[RepoDefinition(provider=\"github\", owner=repo_owner, name=repo_name, external_id=repo_name)],\n",
    "            base_commit_sha=run_item.commit.parents[0].sha,\n",
    "            issue=run_item.issue\n",
    "        )\n",
    "\n",
    "        state = ContinuationState.new(AutofixContinuation(request=AutofixRequest.model_validate(request)), group_id=run_item.issue.id)\n",
    "        try:\n",
    "            codebase_index = CodebaseIndex.from_repo_definition(\n",
    "                run_item.organization_id,\n",
    "                run_item.project_id,\n",
    "                RepoDefinition(provider=\"github\", owner=repo_owner, name=repo_name, external_id=run_item.repo_name),\n",
    "                run_item.commit.parents[0].sha,\n",
    "                None,\n",
    "                state=state,\n",
    "                state_manager_class=AutofixCodebaseStateManager,\n",
    "                embedding_model=None)\n",
    "            summary.append([repo_owner, repo_name, run_item.repo_name, run_item.organization_id, run_item.project_id, run_item.commit.parents[0].sha, 'success'])\n",
    "        except Exception as e:\n",
    "            summary.append([repo_owner, repo_name, run_item.repo_name, run_item.organization_id, run_item.project_id, run_item.commit.parents[0].sha, str(e)])\n",
    "        # [repo_owner, repo_name] = run_item.repo_name.split('/')\n",
    "        \n",
    "        # state = ContinuationState.new(AutofixContinuation(request=AutofixRequest.model_validate(request)), group_id=run_item.issue.id)\n",
    "     \n",
    "        # event_manager = AutofixEventManager(state)\n",
    "        # context = AutofixContext(\n",
    "        #     state=state,\n",
    "        #     sentry_client=rpc_client,\n",
    "        #     event_manager=event_manager,\n",
    "        #     embedding_model=embedding_model,\n",
    "        # )\n",
    "    return pd.DataFrame(summary, columns=['repo_owner', 'repo_name', 'repo_full_name', 'organization_id', 'project_id', \n",
    "                                          'sha', 'error'])\n",
    "        \n",
    "df = check_embeddings()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_queried = df\n",
    "print(df_queried.error.value_counts())\n",
    "print(df_queried.repo_full_name.value_counts())\n",
    "\n",
    "df_queried[df_queried.error=='No repo'].repo_full_name.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, c in df_queried.error.value_counts().items():\n",
    "    if 'No such file or directory:' in v:\n",
    "        path = v.replace(\"[Errno 2] No such file or directory: '\", \"\")\n",
    "        path = path.replace(\"'\", \"\")\n",
    "        print(f'cp -r {path.replace(\"/data/\", \"/old-data/\")} {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation On Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predict function to be called during the eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "embedding_model = SentenceTransformer(\"../models/autofix_embeddings_v0\", trust_remote_code=True)\n",
    "embedding_model.max_seq_length = 4096\n",
    "embedding_model.to(device = torch.device(get_device()))\n",
    "\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    run_item = EvalItem.model_validate(input_)\n",
    "\n",
    "    # Initializes the rpc client in DRY RUN mode\n",
    "    rpc_client = DummyRpcClient()\n",
    "    rpc_client.dry_run = True\n",
    "    [repo_owner, repo_name] = run_item.repo_name.split('/')\n",
    "    request = AutofixRequest(\n",
    "        organization_id=run_item.organization_id,\n",
    "        project_id=run_item.project_id,\n",
    "        repos=[RepoDefinition(provider=\"github\", owner=repo_owner, name=repo_name, external_id=run_item.repo_name)],\n",
    "        base_commit_sha=run_item.commit.parents[0].sha,\n",
    "        issue=run_item.issue,\n",
    "    )\n",
    "    \n",
    "    state = ContinuationState.new(AutofixContinuation(request=AutofixRequest.model_validate(request)), group_id=run_item.issue.id)\n",
    " \n",
    "    event_manager = AutofixEventManager(state)\n",
    "    context = AutofixContext(\n",
    "        state=state,\n",
    "        sentry_client=rpc_client,\n",
    "        event_manager=event_manager,\n",
    "        embedding_model=embedding_model,\n",
    "    )\n",
    "   \n",
    "    AutofixRootCause(context).invoke()\n",
    "    \n",
    "    root_cause_step = state.get().find_step(id='root_cause_analysis')\n",
    "    if not root_cause_step or not isinstance(root_cause_step, RootCauseStep):\n",
    "        return {\"output\": None}\n",
    "    \n",
    "    cause = root_cause_step.causes[0]\n",
    "    if not cause.suggested_fixes:\n",
    "        return {\"output\": None}\n",
    "\n",
    "    event_manager.set_selected_root_cause(SuggestedFixRootCauseSelection(\n",
    "        cause_id=cause.id,\n",
    "        fix_id=cause.suggested_fixes[0].id\n",
    "    ))\n",
    "\n",
    "    AutofixExecution(context).invoke()\n",
    "\n",
    "    changes_step = state.get().find_step(id='changes')\n",
    "    if not changes_step or not isinstance(changes_step, ChangesStep):\n",
    "        return {\"output\": None}\n",
    "    if 'changes' not in changes_step or len(changes_step.changes) == 0:\n",
    "        return {\"output\": None}\n",
    "\n",
    "    return {\"output\": {\n",
    "        \"diff_str\": changes_step.changes[0].diff_str\n",
    "    }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the scoring prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "from seer.automation.autofix.prompts import format_exceptions\n",
    "from seer.automation.autofix.utils import extract_xml_element_text, escape_multi_xml\n",
    "\n",
    "n_panel = 3\n",
    "model = ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0.8)\n",
    "\n",
    "def score_fix_single_it(eval_item: EvalItemWithDiff, predicted_diff_str: str) -> float:\n",
    "    completion = model.invoke(f\"\"\"<issue>\n",
    "<error_message>\n",
    "{eval_item.event.title}\n",
    "</error_message>\n",
    "<exceptions>\n",
    "{format_exceptions(eval_item.event.exceptions)}\n",
    "</exceptions>\n",
    "</issue>\n",
    "\n",
    "Given the above issue, we know the correct fix is:\n",
    "\n",
    "<expected_solution>\n",
    "<description>\n",
    "{eval_item.commit.commit.message}\n",
    "</description>\n",
    "<changes>\n",
    "{eval_item.diff}\n",
    "</changes>\n",
    "</expected_solution>\n",
    "\n",
    "The model outputted the following solution:\n",
    "\n",
    "<predicted_solution>\n",
    "{predicted_diff_str}\n",
    "</predicted_solution>\n",
    "\n",
    "Score how well the predicted solution matches the expected solution with a float score from 0 to 1, where 1 means the solution fully fixes the issue and 0 means the solution does not fix the issue at all.\n",
    "- Consider the context of the issue and the diff\n",
    "- Consider that there are multiple ways to fix an issue\n",
    "\n",
    "Think step-by-step inside a <thoughts> tag before giving a score.\n",
    "Return the score inside a <score> tag.\"\"\")\n",
    "    tree = ET.fromstring(f\"<root>{escape_multi_xml(completion.content, ['score'])}</root>\")\n",
    "    score_str = extract_xml_element_text(tree, 'score')\n",
    "    score = float(score_str) if score_str else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "@traceable(name=\"Score 1 item\", run_type=\"chain\")\n",
    "def score_one(eval_item: EvalItemWithDiff, predicted_diff_str: str) -> float:\n",
    "    return round(sum([score_fix_single_it(eval_item, predicted_diff_str) for _ in range(n_panel)]) / n_panel, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "@run_evaluator\n",
    "def gpt_panel(run: Run, example: Example | None = None):\n",
    "    eval_item = EvalItem.model_validate(run.inputs)\n",
    "    with_diff = EvalItemWithDiff.model_validate(dict(**dict(eval_item), diff=example.outputs.get('diff')))\n",
    "    diff_str = run.outputs.get('output', {}).get('diff_str', None)\n",
    "\n",
    "    if not diff_str:\n",
    "        return EvaluationResult(key=\"diff_gpt_panel_n3_score\", score=None)\n",
    "\n",
    "    score = score_one(with_diff, run.outputs.get('output', {}).get('diff_str', None))\n",
    "    return EvaluationResult(key=\"diff_gpt_panel_n3_score\", score=score)\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[gpt_panel]\n",
    ")\n",
    "\n",
    "ds = langsmith_client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "langsmith_client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=predict_result,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=\"Autofix v2 rev:10\",\n",
    "    concurrency_level=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test to just run one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = langsmith_client.list_examples(dataset_name=dataset_name)\n",
    "\n",
    "for example in examples:\n",
    "    predict_result(example.inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_plots():\n",
    "    examples = langsmith_client.list_examples(dataset_name=dataset_name)\n",
    "    repo_names = [example.inputs['repo_name'] for example in examples]\n",
    "    df_repo_names = pd.DataFrame(repo_names, columns=['repo_name'])\n",
    "    \n",
    "    df_repos_hist = df_repo_names.repo_name.value_counts().sort_values(ascending=False)\n",
    "    sns.set_context(\"paper\", rc={\"font.size\":8,\"axes.titlesize\":14,\"axes.labelsize\":12})\n",
    "    sns.color_palette(\"muted\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.barplot(y=df_repos_hist.index, x=df_repos_hist.values, order=df_repos_hist.index, palette=\"flare\")\n",
    "    sns.despine(fig=None, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
    "    ax.bar_label(ax.containers[0])\n",
    "    plt.title('Resolved Issues With Github Commit')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Github Repo')\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('issues_repos_barplot.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "ds_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Validation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = langsmith_client.get_test_results(project_name='Autofix v2 rev:10')\n",
    "\n",
    "# ['input.repo_name', 'execution_time', 'error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs = df_orig\n",
    "cols_of_interest = ['reference.diff',\n",
    "                    'outputs.output.diff_str',\n",
    "                    'input.repo_name', \n",
    "                    'execution_time', \n",
    "                    'error', \n",
    "                    'id', \n",
    "                    'feedback.diff_gpt_panel_n3_score', \n",
    "                    'input.event.title', \n",
    "                    'input.event.exceptions', \n",
    "                    'input.issue.id', \n",
    "                    'input.issue.title', \n",
    "                    'input.issue.events',\n",
    "                    'input.raw_data.metadata.severity',\n",
    "                    'input.raw_data.metadata.initial_priority',\n",
    "                    'input.raw_data.platform']\n",
    "df_proj = df_runs[cols_of_interest]\n",
    "df_proj.to_csv('last_run.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj.error.str.split('\\n').str.get(0).value_counts(dropna=False).to_frame().style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import table \n",
    "def errors_table():\n",
    "    ax = plt.subplot(111, frame_on=False) # no visible frame\n",
    "    ax.xaxis.set_visible(False)  # hide the x axis\n",
    "    ax.yaxis.set_visible(False)  # hide the y axis\n",
    "    df_errors = df_proj.error.str.split('\\n').str.get(0).to_frame()\n",
    "    # df_errors.loc[df_errors['error'].str. == 0, \"error\"] = 'Successful'\n",
    "    df_errors.error.replace('', 'Successful', inplace=True)\n",
    "    df_errors = df_errors.error.value_counts(dropna=False).to_frame().reset_index()\n",
    "    \n",
    "    params = {'figure.figsize': (12,6),}\n",
    "    plt.rcParams.update(params)\n",
    "    tabla = table(ax, df_errors[['error', 'count']], loc='upper right', colWidths=[.99, 0.1])  # where df is your data frame\n",
    "    tabla.auto_set_font_size(False) # Activate set fontsize manually\n",
    "    tabla.set_fontsize(12) # if ++fontsize is necessary ++colWidths\n",
    "    tabla.scale(1.2, 1.2) # change size table\n",
    "    \n",
    "    # table(ax, df_errors)  # where df is your data frame\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('validation_errors.png', dpi=150)\n",
    "    plt.show()\n",
    "    return df_errors\n",
    "\n",
    "errors_table()    # plt.savefig('mytable.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky fix for runs that error out when we get no fix from the model. In these cases we set the score to 0.\n",
    "df_proj.error = df_proj.error.fillna('')\n",
    "df_proj.loc[(df_proj.error.str.len() == 0), 'failed'] = False\n",
    "df_proj.loc[(df_proj.error.str.startswith(\"IndexError('list index out of range')\")), 'failed'] = False\n",
    "df_proj.loc[(df_proj.failed.isna()), 'failed'] = True\n",
    "df_proj.loc[(df_proj.failed == False) & (df_proj['feedback.diff_gpt_panel_n3_score'].isna()), 'feedback.diff_gpt_panel_n3_score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_successful = df_proj[df_proj.failed == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Execution Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, sharey=True, sharex=False)\n",
    "df_successful.execution_time.plot.hist(title='Execution Times (All)', ax=ax[0], ylabel='')\n",
    "df_successful[df_successful['input.repo_name'].str.startswith('getsentry/')].execution_time.plot.hist(title='Execution Times (Sentry)', ax=ax[1], ylabel='')\n",
    "df_successful[~df_successful['input.repo_name'].str.startswith('getsentry/')].execution_time.plot.hist(title='Execution Times (Open Source)', ax=ax[2], ylabel='')\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=.75, wspace=0.4)\n",
    "sns.set_context(\"paper\", rc={\"font.size\":8,\"axes.titlesize\":14,\"axes.labelsize\":12})\n",
    "params = {\n",
    "          'figure.figsize': (8,6),\n",
    "          'axes.labelsize': 12,\n",
    "          'axes.titlesize': 14,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "# plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "fig.supxlabel('Execution Time (Seconds)')\n",
    "fig.supylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('execution_times.png', dpi=150)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Of Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, sharey=True, sharex=False)\n",
    "df_successful['feedback.diff_gpt_panel_n3_score'].plot.hist(title='GPT Panel Scores (All)', ax=ax[0], ylabel='')\n",
    "df_successful[df_successful['input.repo_name'].str.startswith('getsentry/')]['feedback.diff_gpt_panel_n3_score'].plot.hist(title='GPT Panel Scores (Sentry)', ax=ax[1], ylabel='')\n",
    "df_successful[~df_successful['input.repo_name'].str.startswith('getsentry/')]['feedback.diff_gpt_panel_n3_score'].plot.hist(title='GPT Panel Scores (Open Source)', ax=ax[2], ylabel='')\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=.75, wspace=0.4)\n",
    "sns.set_context(\"paper\", rc={\"font.size\":8,\"axes.titlesize\":14,\"axes.labelsize\":12})\n",
    "params = {\n",
    "          'figure.figsize': (8,6),\n",
    "          'axes.labelsize': 12,\n",
    "          'axes.titlesize': 14,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "# plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "fig.supxlabel('Score')\n",
    "fig.supylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('scores.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_successful[df_successful['feedback.diff_gpt_panel_n3_score'] > 0.0]['feedback.diff_gpt_panel_n3_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
