{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennmueng/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/tqdm-4.66.1-py3.11.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Flask '__main__'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['DATABASE_URL'] = \"postgresql+psycopg://root:seer@localhost:5433/seer\"\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"ai-autofix-evals\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('autofix')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.handlers = []\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "from github import Github\n",
    "from github.Auth import Token\n",
    "\n",
    "github = Github(auth=Token(token=os.environ.get('GITHUB_TOKEN')))\n",
    "repo = github.get_repo('getsentry/sentry')\n",
    "\n",
    "from seer.bootup import bootup\n",
    "\n",
    "bootup(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from pydantic import field_serializer, BaseModel\n",
    "from tqdm import tqdm\n",
    "from github.Commit import Commit\n",
    "from typing import Any, Literal, NotRequired\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import AliasChoices, AliasGenerator, ConfigDict, Field, ValidationError, field_validator\n",
    "from pydantic.alias_generators import to_camel, to_snake\n",
    "import sentry_sdk\n",
    "\n",
    "from seer.automation.autofix.models import IssueDetails, EventDetails\n",
    "\n",
    "class EvalItem(BaseModel):\n",
    "    raw_data: dict[str, Any]\n",
    "    commit: Commit\n",
    "    issue: IssueDetails\n",
    "    event: EventDetails\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True\n",
    "    )\n",
    "\n",
    "    @field_serializer('commit')\n",
    "    def serialize_commit(self, commit: Commit, _info):\n",
    "        return commit.sha\n",
    "    \n",
    "    @field_validator('commit', mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_commit(cls, commit: Commit | str):\n",
    "        return commit if isinstance(commit, Commit) else repo.get_commit(commit)\n",
    "    \n",
    "class EvalItemWithDiff(EvalItem):\n",
    "    diff: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 36 eval items\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "eval_file = '../data/full_eval_autofix_240314.json'\n",
    "\n",
    "with open(eval_file, 'r') as file:\n",
    "    tmp_autofix_data = json.load(file)\n",
    "\n",
    "eval_data = [EvalItem.model_validate(item) for item in tmp_autofix_data]\n",
    "\n",
    "print(f\"Loaded {len(eval_data)} eval items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CELERY_BROKER_URL not set\n"
     ]
    }
   ],
   "source": [
    "from seer.automation.autofix.autofix import Autofix\n",
    "from seer.automation.autofix.tasks import ContinuationState\n",
    "from seer.rpc import DummyRpcClient\n",
    "from seer.automation.autofix.models import (\n",
    "    AutofixContinuation,\n",
    "    AutofixRequest,\n",
    "    IssueDetails,\n",
    "    RepoDefinition,\n",
    ")\n",
    "\n",
    "rpc_client = DummyRpcClient()\n",
    "rpc_client.dry_run = True\n",
    "\n",
    "request = AutofixRequest(\n",
    "    organization_id=1,\n",
    "    project_id=1,\n",
    "    repos=[RepoDefinition(provider=\"github\", owner=\"getsentry\", name=\"sentry\")],\n",
    "    base_commit_sha=eval_data[0].commit.sha,\n",
    "    issue=eval_data[0].issue,\n",
    ")\n",
    "\n",
    "state = ContinuationState(\n",
    "    val=AutofixContinuation(request=AutofixRequest.model_validate(request)), rpc_client=rpc_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from seer.automation.autofix.autofix_context import AutofixContext\n",
    "from seer.automation.autofix.event_manager import AutofixEventManager\n",
    "\n",
    "embedding_model = SentenceTransformer('../models/autofix_embeddings_v0', trust_remote_code=True)\n",
    "\n",
    "event_manager = AutofixEventManager(state)\n",
    "context = AutofixContext(\n",
    "    organization_id=request.organization_id,\n",
    "    project_id=request.project_id,\n",
    "    repos=request.repos,\n",
    "    event_manager=event_manager,\n",
    "    state=state,\n",
    "    embedding_model=embedding_model\n",
    ")\n",
    "context.commit_changes = False\n",
    "autofix = Autofix(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning autofix for issue 5059849041\n",
      "on_autofix_step_update invoking...\n",
      "on_autofix_step_update done\n",
      "on_autofix_step_update invoking...\n",
      "on_autofix_step_update done\n",
      "on_autofix_step_update invoking...\n",
      "on_autofix_step_update done\n",
      "on_autofix_step_update invoking...\n",
      "on_autofix_step_update done\n",
      "Creating codebase index for repo sentry\n",
      "on_autofix_step_update invoking...\n",
      "on_autofix_step_update done\n",
      "Loading repository to /var/folders/c8/ljt8gc_13j30r7lt_p842hrw0000gn/T/getsentry-sentry_21831a3a2935e295dfdcf40f800c77119fd90886m3gdbpjj/repo\n",
      "Loaded repository to /var/folders/c8/ljt8gc_13j30r7lt_p842hrw0000gn/T/getsentry-sentry_21831a3a2935e295dfdcf40f800c77119fd90886m3gdbpjj/repo\n",
      "Read 11683 documents:\n",
      "  markdown: 21\n",
      "  yaml: 52\n",
      "  python: 5783\n",
      "  json: 646\n",
      "  bash: 14\n",
      "  typescript: 171\n",
      "  toml: 1\n",
      "  tsx: 4765\n",
      "  javascript: 47\n",
      "  rst: 9\n",
      "  html: 162\n",
      "  css: 2\n",
      "  lua: 7\n",
      "  embedded_template: 3\n",
      "Document chunking took 0.73 seconds\n",
      "Processed document 1/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 2/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 3/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 4/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 5/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 6/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 7/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 8/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 9/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 10/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 11/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 12/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 13/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 14/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 15/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 16/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 17/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 18/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 19/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 20/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 21/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 22/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 23/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 24/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 25/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 26/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 27/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 28/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 29/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 30/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 31/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 32/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 33/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 34/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 35/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 36/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 37/11683\n",
      "Document chunking took 0.05 seconds\n",
      "Processed document 38/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 39/11683\n",
      "Document chunking took 0.15 seconds\n",
      "Processed document 40/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 41/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 42/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 43/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 44/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 45/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 46/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 47/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 48/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 49/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 50/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 51/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 52/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 53/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 54/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 55/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 56/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 57/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 58/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 59/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 60/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 61/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 62/11683\n",
      "Document chunking took 0.04 seconds\n",
      "Processed document 63/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 64/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 65/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 66/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 67/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 68/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 69/11683\n",
      "Document chunking took 0.25 seconds\n",
      "Processed document 70/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 71/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 72/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 73/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 74/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 75/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 76/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 77/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 78/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 79/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 80/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 81/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 82/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 83/11683\n",
      "Document chunking took 0.38 seconds\n",
      "Processed document 84/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 85/11683\n",
      "Document chunking took 0.48 seconds\n",
      "Processed document 86/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 87/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 88/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 89/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 90/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 91/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 92/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 93/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 94/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 95/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 96/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 97/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 98/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 99/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 100/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 101/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 102/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 103/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 104/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 105/11683\n",
      "Document chunking took 0.58 seconds\n",
      "Processed document 106/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 107/11683\n",
      "Document chunking took 0.06 seconds\n",
      "Processed document 108/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 109/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 110/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 111/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 112/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 113/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 114/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 115/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 116/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 117/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 118/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 119/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 120/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 121/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 122/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 123/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 124/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 125/11683\n",
      "Document chunking took 0.11 seconds\n",
      "Processed document 126/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 127/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 128/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 129/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 130/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 131/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 132/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 133/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 134/11683\n",
      "Document chunking took 0.04 seconds\n",
      "Processed document 135/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 136/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 137/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 138/11683\n",
      "Document chunking took 0.13 seconds\n",
      "Processed document 139/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 140/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 141/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 142/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 143/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 144/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 145/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 146/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 147/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 148/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 149/11683\n",
      "Document chunking took 0.10 seconds\n",
      "Processed document 150/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 151/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 152/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 153/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 154/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 155/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 156/11683\n",
      "Document chunking took 0.04 seconds\n",
      "Processed document 157/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 158/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 159/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 160/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 161/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 162/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 163/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 164/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 165/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 166/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 167/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 168/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 169/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 170/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 171/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 172/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 173/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 174/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 175/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 176/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 177/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 178/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 179/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 180/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 181/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 182/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 183/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 184/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 185/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 186/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 187/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 188/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 189/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 190/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 191/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 192/11683\n",
      "Document chunking took 0.05 seconds\n",
      "Processed document 193/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 194/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 195/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 196/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 197/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 198/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 199/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 200/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 201/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 202/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 203/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 204/11683\n",
      "Document chunking took 0.11 seconds\n",
      "Processed document 205/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 206/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 207/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 208/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 209/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 210/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 211/11683\n",
      "Document chunking took 0.06 seconds\n",
      "Processed document 212/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 213/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 214/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 215/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 216/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 217/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 218/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 219/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 220/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 221/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 222/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 223/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 224/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 225/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 226/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 227/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 228/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 229/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 230/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 231/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 232/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 233/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 234/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 235/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 236/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 237/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 238/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 239/11683\n",
      "Document chunking took 0.04 seconds\n",
      "Processed document 240/11683\n",
      "Document chunking took 0.06 seconds\n",
      "Processed document 241/11683\n",
      "Document chunking took 0.01 seconds\n",
      "Processed document 242/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 243/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 244/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 245/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 246/11683\n",
      "Document chunking took 0.03 seconds\n",
      "Processed document 247/11683\n",
      "Document chunking took 0.02 seconds\n",
      "Processed document 248/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 249/11683\n",
      "Document chunking took 0.00 seconds\n",
      "Processed document 250/11683\n",
      "Autofix complete for issue 5059849041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Runs the autofix run\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mautofix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py:453\u001b[0m, in \u001b[0;36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[0;34m(langsmith_extra, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[1;32m    452\u001b[0m     _container_end(run_container, error\u001b[38;5;241m=\u001b[39mstacktrace)\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     _PARENT_RUN_TREE\u001b[38;5;241m.\u001b[39mset(context_run)\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py:449\u001b[0m, in \u001b[0;36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[0;34m(langsmith_extra, *args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m         function_result \u001b[38;5;241m=\u001b[39m func(\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;241m*\u001b[39margs, run_tree\u001b[38;5;241m=\u001b[39mrun_container[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_run\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    447\u001b[0m         )\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m         function_result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/autofix/autofix.py:85\u001b[0m, in \u001b[0;36mAutofix.invoke\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sentry_sdk\u001b[38;5;241m.\u001b[39mstart_span(\n\u001b[1;32m     81\u001b[0m     op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseer.automation.autofix.codebase_index.create\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate codebase index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m     84\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m, repo\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_codebase_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m autofix_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodebase index created for repo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mevent_manager\u001b[38;5;241m.\u001b[39msend_codebase_index_creation_complete_message(\n\u001b[1;32m     88\u001b[0m     repo\u001b[38;5;241m.\u001b[39mfull_name\n\u001b[1;32m     89\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/autofix/autofix_context.py:58\u001b[0m, in \u001b[0;36mAutofixContext.create_codebase_index\u001b[0;34m(self, repo)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_codebase_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, repo: RepoDefinition):\n\u001b[0;32m---> 58\u001b[0m     codebase_index \u001b[38;5;241m=\u001b[39m \u001b[43mCodebaseIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morganization_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebases[codebase_index\u001b[38;5;241m.\u001b[39mrepo_info\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m codebase_index\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py:453\u001b[0m, in \u001b[0;36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[0;34m(langsmith_extra, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[1;32m    452\u001b[0m     _container_end(run_container, error\u001b[38;5;241m=\u001b[39mstacktrace)\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     _PARENT_RUN_TREE\u001b[38;5;241m.\u001b[39mset(context_run)\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py:449\u001b[0m, in \u001b[0;36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[0;34m(langsmith_extra, *args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m         function_result \u001b[38;5;241m=\u001b[39m func(\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;241m*\u001b[39margs, run_tree\u001b[38;5;241m=\u001b[39mrun_container[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_run\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    447\u001b[0m         )\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m         function_result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/codebase_index.py:161\u001b[0m, in \u001b[0;36mCodebaseIndex.create\u001b[0;34m(cls, organization, project, repo, run_id, embedding_model)\u001b[0m\n\u001b[1;32m    159\u001b[0m doc_parser \u001b[38;5;241m=\u001b[39m DocumentParser(embedding_model)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sentry_sdk\u001b[38;5;241m.\u001b[39mstart_span(op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseer.automation.codebase.create.process_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 161\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43mdoc_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sentry_sdk\u001b[38;5;241m.\u001b[39mstart_span(op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseer.automation.codebase.create.embed_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    163\u001b[0m     embedded_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_chunks(chunks, embedding_model)\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:241\u001b[0m, in \u001b[0;36mDocumentParser.process_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    239\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, document \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents):\n\u001b[0;32m--> 241\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    242\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:230\u001b[0m, in \u001b[0;36mDocumentParser.process_document\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03mProcess a document by chunking it into smaller pieces and extracting metadata about each chunk.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m start_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 230\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument chunking took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:198\u001b[0m, in \u001b[0;36mDocumentParser._chunk_document\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chunk_document\u001b[39m(\u001b[38;5;28mself\u001b[39m, document: Document) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[BaseDocumentChunk]:\n\u001b[1;32m    196\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parser(document\u001b[38;5;241m.\u001b[39mlanguage)\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;28mbytes\u001b[39m(document\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 198\u001b[0m     chunked_documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_nodes_by_whitespace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     chunks: \u001b[38;5;28mlist\u001b[39m[BaseDocumentChunk] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tmp_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunked_documents):\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:137\u001b[0m, in \u001b[0;36mDocumentParser._chunk_nodes_by_whitespace\u001b[0;34m(self, node, language, parent_declarations, root_node, last_end_byte)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m declaration:\n\u001b[1;32m    135\u001b[0m         parent_declarations_for_children\u001b[38;5;241m.\u001b[39mappend(declaration)\n\u001b[0;32m--> 137\u001b[0m children_with_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_nodes_by_whitespace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_declarations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_declarations_for_children\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_end_byte\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_end_byte\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(children_with_embeddings) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# This case is for when the first chunk of the children is touching the last chunk of the current chunks\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Usually when the definition of the parent is split from its children\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# This combines the first logical chunk with its parent definition line.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_touching_last(children_with_embeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:137\u001b[0m, in \u001b[0;36mDocumentParser._chunk_nodes_by_whitespace\u001b[0;34m(self, node, language, parent_declarations, root_node, last_end_byte)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m declaration:\n\u001b[1;32m    135\u001b[0m         parent_declarations_for_children\u001b[38;5;241m.\u001b[39mappend(declaration)\n\u001b[0;32m--> 137\u001b[0m children_with_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_nodes_by_whitespace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_declarations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_declarations_for_children\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_end_byte\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_end_byte\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(children_with_embeddings) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# This case is for when the first chunk of the children is touching the last chunk of the current chunks\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Usually when the definition of the parent is split from its children\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# This combines the first logical chunk with its parent definition line.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_touching_last(children_with_embeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;241m0\u001b[39m]):\n",
      "    \u001b[0;31m[... skipping similar frames: DocumentParser._chunk_nodes_by_whitespace at line 137 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:137\u001b[0m, in \u001b[0;36mDocumentParser._chunk_nodes_by_whitespace\u001b[0;34m(self, node, language, parent_declarations, root_node, last_end_byte)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m declaration:\n\u001b[1;32m    135\u001b[0m         parent_declarations_for_children\u001b[38;5;241m.\u001b[39mappend(declaration)\n\u001b[0;32m--> 137\u001b[0m children_with_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_nodes_by_whitespace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_declarations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_declarations_for_children\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_end_byte\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_end_byte\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(children_with_embeddings) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# This case is for when the first chunk of the children is touching the last chunk of the current chunks\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Usually when the definition of the parent is split from its children\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# This combines the first logical chunk with its parent definition line.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_touching_last(children_with_embeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:126\u001b[0m, in \u001b[0;36mDocumentParser._chunk_nodes_by_whitespace\u001b[0;34m(self, node, language, parent_declarations, root_node, last_end_byte)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(children)):\n\u001b[1;32m    123\u001b[0m     potential_chunk \u001b[38;5;241m=\u001b[39m TempChunk(\n\u001b[1;32m    124\u001b[0m         nodes\u001b[38;5;241m=\u001b[39m[children[i]], parent_declarations\u001b[38;5;241m=\u001b[39mparent_declarations\n\u001b[1;32m    125\u001b[0m     )\n\u001b[0;32m--> 126\u001b[0m     token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chunk_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpotential_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreak_chunks_at:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# Recursively chunk the children if the current node is too big or should be chunked\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         parent_declarations_for_children \u001b[38;5;241m=\u001b[39m parent_declarations\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:87\u001b[0m, in \u001b[0;36mDocumentParser._get_chunk_tokens\u001b[0;34m(self, chunk, root)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_chunk_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, chunk: TempChunk, root: Node) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_str_token_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dump_for_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/src/seer/automation/codebase/parser.py:81\u001b[0m, in \u001b[0;36mDocumentParser._get_str_token_count\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_str_token_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:461\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]):\n\u001b[1;32m    458\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:146\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n\u001b[1;32m    143\u001b[0m     to_tokenize \u001b[38;5;241m=\u001b[39m [[s\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m to_tokenize]\n\u001b[1;32m    145\u001b[0m output\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_tokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongest_first\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2802\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2803\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2889\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2884\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2885\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2886\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2887\u001b[0m         )\n\u001b[1;32m   2888\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2910\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2911\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2927\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2928\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3080\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3071\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3072\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3073\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3077\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3078\u001b[0m )\n\u001b[0;32m-> 3080\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:270\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m )\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/timeseries-analysis-service/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Runs the autofix run\n",
    "autofix_result = autofix.invoke(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP scoring the diffs\n",
    "\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from github.Commit import Commit\n",
    "from github.File import File\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
    "\n",
    "def score_fix(error_message, stacktrace, expected_solution, expected_diff, predicted_diff):\n",
    "    model.invoke(f\"\"\"<issue>\n",
    "{error_message}\n",
    "{stacktrace}\n",
    "</issue>\n",
    "\n",
    "Given the above issue, we know the correct fix is:\n",
    "\n",
    "<expected_solution>\n",
    "<description>\n",
    "{expected_solution}\n",
    "</description>\n",
    "<changes>\n",
    "{expected_diff}\n",
    "</changes>\n",
    "</expected_solution>\n",
    "\n",
    "The model predicted the following diff:\n",
    "\n",
    "<predicted_solution>\n",
    "{predicted_diff}\n",
    "</predicted_solution>\n",
    "\n",
    "Score how well the predicted solution matches the expected solution with a float score from 0 to 1, where 1 means the solution fully fixes the issue and 0 means the solution does not fix the issue at all.\n",
    "- Consider the context of the issue and the diff\n",
    "- Consider that there are multiple ways to fix an issue\n",
    "- Return the score inside a <score> tag.\"\"\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
